{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "steelpan-crepe.ipynb.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "interpreter": {
      "hash": "e92005438139a4419431294f52efbb345a0cf152313ea7bfec8e749b14196e5a"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XZ5vVsiSNaUN"
      },
      "source": [
        "# CREPE Trainer\n",
        "# I'm just putting this here for backup now, vscode has done some weird things with messing up my files\n",
        "\n",
        "This notebook can create CREPE models and train them on steelpan data.\n",
        "\n",
        "To use the notebook as is, download the downsampled audio (\"tiny_16kHz/\") and make sure that the dirpaths for the train and validation sets are correct. Also download the .h5 files from the CREPE repo's \"models\" branch if you want to train from existing weights."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GNU0hEyjM7QZ",
        "outputId": "dfa5cc37-f4dd-42c5-d7a4-2a223c523b4f"
      },
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "from numpy.lib.stride_tricks import as_strided\n",
        "import os\n",
        "from os import walk\n",
        "import soundfile\n",
        "import librosa\n",
        "\n",
        "print(tf.__version__)\n",
        "# This code allows for the GPU to be utilized properly.\n",
        "tf.autograph.set_verbosity(0)\n",
        "physical_devices = tf.config.list_physical_devices(\"GPU\")\n",
        "try:\n",
        "    tf.config.experimental.set_memory_growth(physical_devices[0], True)\n",
        "except:\n",
        "    pass\n",
        "\n",
        "print(physical_devices)\n",
        "print(\"If the above list is empty, then TF won't use any accelerator\")"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.7.0\n",
            "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n",
            "If the above list is empty, then TF won't use any accelerator\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vWCnYvL68l_k",
        "outputId": "97a3dbbb-f601-47cf-be27-f40057ca1ad3"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0icYDMG8PZ1I"
      },
      "source": [
        "## Model builder\n",
        "\n",
        "This code is modified a bit from the repo as it normally stores the models in a dict, which I think is unnecessary for our purposes.\n",
        "\n",
        "You can also load weights from an existing .h5 file. I began training by loading the weights of \"model-full.h5\" from the marl/crepe models branch. The weights for the model trained on steelpan data is named \"new-crepe-full.h5\" and is in the Drive folder."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pamdqnF5M7Qe"
      },
      "source": [
        "# store as a global variable, since we only support a few models for now\n",
        "models = {\n",
        "    'tiny': None,\n",
        "    'small': None,\n",
        "    'medium': None,\n",
        "    'large': None,\n",
        "    'full': None\n",
        "}\n",
        "\n",
        "# the model is trained on 16kHz audio\n",
        "model_srate = 16000\n",
        "\n",
        "def make_model(model_capacity, metrics, weights=None):\n",
        "    '''\n",
        "    model_capacity: tiny, small, medium, large, full\n",
        "    weights: path of .h5 weights file\n",
        "    '''\n",
        "\n",
        "    from tensorflow.keras.layers import Input, Reshape, Conv2D, BatchNormalization\n",
        "    from tensorflow.keras.layers import MaxPool2D, Dropout, Permute, Flatten, Dense\n",
        "    from tensorflow.keras.models import Model\n",
        "\n",
        "    capacity_multiplier = {\n",
        "        'tiny': 4, 'small': 8, 'medium': 16, 'large': 24, 'full': 32\n",
        "    }[model_capacity]\n",
        "\n",
        "    layers = [1, 2, 3, 4, 5, 6]\n",
        "    filters = [n * capacity_multiplier for n in [32, 4, 4, 4, 8, 16]]\n",
        "    widths = [512, 64, 64, 64, 64, 64]\n",
        "    strides = [(4, 1), (1, 1), (1, 1), (1, 1), (1, 1), (1, 1)]\n",
        "\n",
        "    x = Input(shape=(1024,), name='input', dtype='float32')\n",
        "    y = Reshape(target_shape=(1024, 1, 1), name='input-reshape')(x)\n",
        "\n",
        "    for l, f, w, s in zip(layers, filters, widths, strides):\n",
        "        y = Conv2D(f, (w, 1), strides=s, padding='same',\n",
        "                    activation='relu', name=\"conv%d\" % l)(y)\n",
        "        y = BatchNormalization(name=\"conv%d-BN\" % l)(y)\n",
        "        y = MaxPool2D(pool_size=(2, 1), strides=None, padding='valid',\n",
        "                        name=\"conv%d-maxpool\" % l)(y)\n",
        "        y = Dropout(0.25, name=\"conv%d-dropout\" % l)(y)\n",
        "\n",
        "    y = Permute((2, 1, 3), name=\"transpose\")(y)\n",
        "    y = Flatten(name=\"flatten\")(y)\n",
        "    y = Dense(360, activation='sigmoid', name=\"classifier\")(y)\n",
        "\n",
        "    model = Model(inputs=x, outputs=y)\n",
        "\n",
        "    if weights != None:\n",
        "        model.load_weights(weights)\n",
        "    model.compile(tf.keras.optimizers.Adam(learning_rate=0.0002), 'binary_crossentropy', metrics=metrics)\n",
        "\n",
        "    models[model_capacity] = model\n",
        "\n",
        "    return model"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SIx7SuJTnzD8"
      },
      "source": [
        "# todo - Just use Librosa's db_to_power instead?\n",
        "def db_to_pow(db):\n",
        "  '''Convert from dB to power'''\n",
        "  return 10**(db / 10)\n",
        "\n",
        "\n",
        "def frame_energy(frame):\n",
        "  '''Calculates the average energy for a frame\n",
        "    \n",
        "    Parameters\n",
        "    ----------\n",
        "    frame : np.array\n",
        "      audio frame in np.float32 format\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    average_energy : float\n",
        "      Average energy level for frame\n",
        "  '''\n",
        "\n",
        "  # Square the sample values to convert to energy values\n",
        "  energy = frame**2\n",
        "\n",
        "  # Sum the energy values to get total energy\n",
        "  total_energy = np.sum(energy)\n",
        "\n",
        "  # Divide by length to get average energy\n",
        "  return total_energy / len(frame)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LUIdeIZOQoTJ"
      },
      "source": [
        "## Data formatting\n",
        "\n",
        "Audio is formatted according to how CREPE does it, but is formatted into batch format for input into model.fit(). Step size defaults to 10ms like in CREPE.\n",
        "\n",
        "The label data is scaled and encoded into one-hots to fit the model's bucketed output. There are 360 outputs, but only 29 actually appear as labels (MIDI pitches 60-89).\n",
        "\n",
        "Data is being trimmed by dBFS levels, which I've set to -30 for now. Anything -40 and below trims almost nothing, and -30 trims a good amount (I think it does need more tuning). **No, trimming needs to happen before normalization. That's the issue.**\n",
        "\n",
        "The one-hotted labels have Gaussian blurring applied to them too (as specified in the other notebook)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UMdS1JKxM7Qf",
        "outputId": "f710f371-38c5-463f-c7b2-f69ae4c468f8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 147
        }
      },
      "source": [
        "'''# todo - fix this\n",
        "# The latest changes have broken the dimensions of the output frames (1024,)\n",
        "# The labels appear accurate, but check it\n",
        "\n",
        "def load_audio_batch(dir, model_srate=16000, threshold_db=-60, step_size=10, blur=True):\n",
        "    '''Load a batch of audio files \n",
        "    \n",
        "    Parameters\n",
        "    ----------\n",
        "    dir : str\n",
        "        Filepath of directory containing audio files to load\n",
        "    threshold_db : int\n",
        "        Threshold for cutting leading and trailing silence of audio files in dB\n",
        "    step_size : float\n",
        "        Step size between audio frames in ms\n",
        "    \n",
        "    Returns\n",
        "    ------\n",
        "    audio_frames : np.ndarray[shape=(1024,360,#todo)]\n",
        "    labels : np.ndarray[shape=(360,)]\n",
        "    '''\n",
        "    \n",
        "    from numpy.lib.stride_tricks import as_strided\n",
        "    \n",
        "    # Initialize arrays\n",
        "    audio_frames = np.empty((0,1024), dtype=np.float32)\n",
        "    audio_labels = []\n",
        "\n",
        "    hop_length = int(model_srate * step_size / 1000)\n",
        "    threshold_pow = db_to_pow(threshold_db)\n",
        "\n",
        "    #test\n",
        "    count = 0\n",
        "    # Iterate over files in directory\n",
        "    for (dirpath, dirnames, filenames) in os.walk(dir):\n",
        "        for filename in filenames:\n",
        "            # load audio\n",
        "            audio, _ = soundfile.read(dirpath + \"/\" + filename)\n",
        "            \n",
        "            \n",
        "            # Split into audio frames\n",
        "            n_frames = 1 + int((len(audio) - 1024) / hop_length)\n",
        "            framed_audio = as_strided(audio, shape=(1024, n_frames),\n",
        "                                strides=(audio.itemsize, hop_length * audio.itemsize))\n",
        "            framed_audio = framed_audio.transpose().copy()\n",
        "            \n",
        "            \n",
        "             # Trim audio leading and trailing silence from audio\n",
        "            for f in range(len(framed_audio)):\n",
        "                if frame_energy(framed_audio[f]) > threshold_pow:\n",
        "                    start_frame = f\n",
        "                    break\n",
        "\n",
        "            for f in range(len(framed_audio) - 1, -1, -1):\n",
        "                if frame_energy(framed_audio[f]) > threshold_pow:\n",
        "                    end_frame = f + 1\n",
        "                    break\n",
        "                    \n",
        "            trimmed_audio = framed_audio[start_frame:end_frame]\n",
        "\n",
        "            \n",
        "            # Normalize the audio data by frame\n",
        "            trimmed_audio -= np.mean(trimmed_audio, axis=1)[:, np.newaxis]\n",
        "            trimmed_audio /= np.std(trimmed_audio, axis=1)[:, np.newaxis]\n",
        "            \n",
        "            # Append normalized audio to audio_frames array\n",
        "            audio_frames = np.append(audio_frames, trimmed_audio, axis=0)\n",
        "            \n",
        "            \n",
        "            \n",
        "            # Append values to the labels array\n",
        "            audio_labels += [int(filename.split(\"_\")[0]) for _ in range(end_frame - start_frame)]\n",
        "            \n",
        "            \n",
        "    \n",
        "    # Convert audio_labels to numpy array\n",
        "    audio_labels = np.array(tf.one_hot(5 * (np.array(audio_labels) - 24), 360))\n",
        "    \n",
        "    if blur:\n",
        "        # Apply Gaussian blur to labels\n",
        "        cents_i = np.arange(360)\n",
        "        for i in range(len(audio_labels)):\n",
        "            cents_true = np.where(audio_labels[i] == 1)[0][0]\n",
        "            audio_labels[i] = np.exp(-((20 *(cents_i - cents_true)) ** 2) / (2 * (25 ** 2)))\n",
        "\n",
        "        \n",
        "    return audio_frames, audio_labels'''"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-5-f0026627b6b3>\"\u001b[0;36m, line \u001b[0;32m6\u001b[0m\n\u001b[0;31m    '''Load a batch of audio files\u001b[0m\n\u001b[0m                                   \n^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fj-HqRO6dU3S"
      },
      "source": [
        "def process_audio_file(filepath, threshold_pow, step_size=10):\n",
        "    '''\n",
        "        Processes a single audio file by formatting it into frames and extracting the label.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        filepath : string\n",
        "        Filepath for audio.\n",
        "\n",
        "        threshold_pow : float\n",
        "        Frames with average energy less than this are clipped.\n",
        "\n",
        "        step_size : int\n",
        "        Space between each frame (in milliseconds).\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        frames : np.array dtype=float\n",
        "        The normalized and clipped audio, in frame form.\n",
        "\n",
        "        labels : int[]\n",
        "        The pitch of the audio file, in MIDI. It's duplicated\n",
        "        because the model requires one label for each frame.\n",
        "    '''\n",
        "    audio, _ = soundfile.read(filepath)\n",
        "\n",
        "    # Format the audio data into frames\n",
        "    hop_length = int(model_srate * step_size / 1000)\n",
        "    n_frames = 1 + int((len(audio) - 1024) / hop_length)\n",
        "    frames = as_strided(audio, shape=(1024, n_frames),\n",
        "                        strides=(audio.itemsize, hop_length * audio.itemsize))\n",
        "    frames = frames.transpose().copy()\n",
        "\n",
        "    # Find the first and last frames where levels meet threshold_pow\n",
        "    for f in range(len(frames)):\n",
        "        if frame_energy(frames[f]) > threshold_pow:\n",
        "            start_frame = f\n",
        "            break\n",
        "\n",
        "    for f in range(len(frames) - 1, -1, -1):\n",
        "        if frame_energy(frames[f]) > threshold_pow:\n",
        "            end_frame = f + 1\n",
        "            break\n",
        "\n",
        "    frames = frames[start_frame:end_frame]\n",
        "\n",
        "    # Normalize each frame\n",
        "    frames -= np.mean(frames, axis=1)[:, np.newaxis]\n",
        "    frames /= np.std(frames, axis=1)[:, np.newaxis]\n",
        "\n",
        "    # Duplicate the label for these frames the set amount of times for the target data\n",
        "    labels = [int(os.path.basename(filepath).split(\"_\")[0]) for _ in range(end_frame - start_frame)]\n",
        "\n",
        "    return frames, labels"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zonzg4t1dUvb"
      },
      "source": [
        "def load_audio_batch(dir, threshold_db, blur=True):\n",
        "    '''\n",
        "        Loads and processes all audio files from a directory, returning a single\n",
        "        array of audio frames and labels. Will load audio from nested dirs as well.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        dir : string\n",
        "        Starting dirpath to load audio from.\n",
        "\n",
        "        threshold_db : float\n",
        "        Frames with average energy less than this are clipped (this is converted dB -> power first).\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        frames_list : np.array dtype=float\n",
        "        Audio frames from all loaded audio files.\n",
        "\n",
        "        labels_list : np.array dtype=float\n",
        "        The labels for the model. It is formatted as a one-hot vector\n",
        "        with Gaussian blur applied, so indices closer to the correct\n",
        "        index are closer to 1 (e.g. if the correct index is 200 then\n",
        "        index 198 might be 0.953 but index 50 might be 0.067).\n",
        "    '''\n",
        "\n",
        "    frames_list = []\n",
        "    labels_list = []\n",
        "    threshold_pow = db_to_pow(threshold_db)\n",
        "\n",
        "    for (dirpath, _, filenames) in walk(dir):\n",
        "        for filename in filenames:\n",
        "            frames, labels = process_audio_file(dirpath + \"/\" + filename, threshold_pow)\n",
        "\n",
        "            frames_list.append(frames)\n",
        "            labels_list += labels\n",
        "\n",
        "    # Assemble frames from all files into one array\n",
        "    frames_list = np.concatenate(frames_list)\n",
        "\n",
        "    # Convert MIDI to CREPE cents one-hot vector (0-360)\n",
        "    with tf.device('/cpu:0'):\n",
        "      labels_list = np.array(tf.one_hot(5 * (np.array(labels_list) - 24), 360))\n",
        "\n",
        "    # Apply Gaussian blur\n",
        "    if blur == True:\n",
        "      cents_i = np.arange(360)\n",
        "      for i in range(len(labels_list)):\n",
        "          cents_true = np.where(labels_list[i] == 1)[0][0]\n",
        "          labels_list[i] = np.exp(-((20 *(cents_i - cents_true)) ** 2) / (2 * (25 ** 2)))\n",
        "\n",
        "    return frames_list, labels_list"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r2zhlSGiM7Qg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "993afd10-d206-4a75-cbd9-779c9eb53473"
      },
      "source": [
        "metrics = \"Accuracy\"\n",
        "model = make_model(\"full\", metrics=metrics, weights='/content/drive/MyDrive/Research Projects/Steelpan pitch detection/Jason\\'s Work/model-full.h5')\n",
        "model.summary()"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input (InputLayer)          [(None, 1024)]            0         \n",
            "                                                                 \n",
            " input-reshape (Reshape)     (None, 1024, 1, 1)        0         \n",
            "                                                                 \n",
            " conv1 (Conv2D)              (None, 256, 1, 1024)      525312    \n",
            "                                                                 \n",
            " conv1-BN (BatchNormalizatio  (None, 256, 1, 1024)     4096      \n",
            " n)                                                              \n",
            "                                                                 \n",
            " conv1-maxpool (MaxPooling2D  (None, 128, 1, 1024)     0         \n",
            " )                                                               \n",
            "                                                                 \n",
            " conv1-dropout (Dropout)     (None, 128, 1, 1024)      0         \n",
            "                                                                 \n",
            " conv2 (Conv2D)              (None, 128, 1, 128)       8388736   \n",
            "                                                                 \n",
            " conv2-BN (BatchNormalizatio  (None, 128, 1, 128)      512       \n",
            " n)                                                              \n",
            "                                                                 \n",
            " conv2-maxpool (MaxPooling2D  (None, 64, 1, 128)       0         \n",
            " )                                                               \n",
            "                                                                 \n",
            " conv2-dropout (Dropout)     (None, 64, 1, 128)        0         \n",
            "                                                                 \n",
            " conv3 (Conv2D)              (None, 64, 1, 128)        1048704   \n",
            "                                                                 \n",
            " conv3-BN (BatchNormalizatio  (None, 64, 1, 128)       512       \n",
            " n)                                                              \n",
            "                                                                 \n",
            " conv3-maxpool (MaxPooling2D  (None, 32, 1, 128)       0         \n",
            " )                                                               \n",
            "                                                                 \n",
            " conv3-dropout (Dropout)     (None, 32, 1, 128)        0         \n",
            "                                                                 \n",
            " conv4 (Conv2D)              (None, 32, 1, 128)        1048704   \n",
            "                                                                 \n",
            " conv4-BN (BatchNormalizatio  (None, 32, 1, 128)       512       \n",
            " n)                                                              \n",
            "                                                                 \n",
            " conv4-maxpool (MaxPooling2D  (None, 16, 1, 128)       0         \n",
            " )                                                               \n",
            "                                                                 \n",
            " conv4-dropout (Dropout)     (None, 16, 1, 128)        0         \n",
            "                                                                 \n",
            " conv5 (Conv2D)              (None, 16, 1, 256)        2097408   \n",
            "                                                                 \n",
            " conv5-BN (BatchNormalizatio  (None, 16, 1, 256)       1024      \n",
            " n)                                                              \n",
            "                                                                 \n",
            " conv5-maxpool (MaxPooling2D  (None, 8, 1, 256)        0         \n",
            " )                                                               \n",
            "                                                                 \n",
            " conv5-dropout (Dropout)     (None, 8, 1, 256)         0         \n",
            "                                                                 \n",
            " conv6 (Conv2D)              (None, 8, 1, 512)         8389120   \n",
            "                                                                 \n",
            " conv6-BN (BatchNormalizatio  (None, 8, 1, 512)        2048      \n",
            " n)                                                              \n",
            "                                                                 \n",
            " conv6-maxpool (MaxPooling2D  (None, 4, 1, 512)        0         \n",
            " )                                                               \n",
            "                                                                 \n",
            " conv6-dropout (Dropout)     (None, 4, 1, 512)         0         \n",
            "                                                                 \n",
            " transpose (Permute)         (None, 1, 4, 512)         0         \n",
            "                                                                 \n",
            " flatten (Flatten)           (None, 2048)              0         \n",
            "                                                                 \n",
            " classifier (Dense)          (None, 360)               737640    \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 22,244,328\n",
            "Trainable params: 22,239,976\n",
            "Non-trainable params: 4,352\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uP4v_bqtSMWe"
      },
      "source": [
        "## Loading/Formatting data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NIt9nLASdy3N"
      },
      "source": [
        "x_train, y_train = load_audio_batch('/content/drive/MyDrive/Research Projects/Steelpan pitch detection/Jason\\'s Work/tiny_16kHz/train', -60, blur=True)\n",
        "x_val, y_val = load_audio_batch('/content/drive/MyDrive/Research Projects/Steelpan pitch detection/Jason\\'s Work/tiny_16kHz/validation', -60, blur=False)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bbCYJaC4Sio2"
      },
      "source": [
        "## Training\n",
        "\n",
        "Any batch size larger than 256 is slower, and also has trouble fitting into VRAM. 20 epochs was an arbitrary choice, which took roughly an hour and a half on a 1650 super.\n",
        "\n",
        "This also saves the model's weights after training."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jyDFC64UM7Qi",
        "pycharm": {
          "is_executing": true
        },
        "scrolled": false,
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 426
        },
        "outputId": "3d0dc70d-1509-46e3-c2bc-43a84ccdbc5c"
      },
      "source": [
        "# todo - Increasing the batch size slows convergence. The original model was trained on 32 sample batches. Is there difference in performance depending on the batch size?\n",
        "# Batch size will effect batch normalization, backpropagation, etc...\n",
        "# Anecdotally, it seemed that I got better validation accuracy scores using batch_size=128\n",
        "history = model.fit(x=x_train, y=y_train, batch_size=32, epochs=1, validation_data=(x_val, y_val))\n",
        "model.save_weights('test-crepe-full_colab_10.h5')"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "2424/2424 [==============================] - 502s 194ms/step - loss: 0.0126 - Accuracy: 0.8163 - val_loss: 0.0114 - val_Accuracy: 0.9671\n",
            "Epoch 2/10\n",
            " 386/2424 [===>..........................] - ETA: 4:59 - loss: 0.0091 - Accuracy: 0.9854"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-11-50c8976a2f88>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# Batch size will effect batch normalization, backpropagation, etc...\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# Anecdotally, it seemed that I got better validation accuracy scores using batch_size=128\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'test-crepe-full_colab_10.h5'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1219\u001b[0m               \u001b[0mlogs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtmp_logs\u001b[0m  \u001b[0;31m# No error, now safe to assign to logs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1220\u001b[0m               \u001b[0mend_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep_increment\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1221\u001b[0;31m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mend_step\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1222\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_training\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1223\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/callbacks.py\u001b[0m in \u001b[0;36mon_train_batch_end\u001b[0;34m(self, batch, logs)\u001b[0m\n\u001b[1;32m    434\u001b[0m     \"\"\"\n\u001b[1;32m    435\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_should_call_train_batch_hooks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 436\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_batch_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mModeKeys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'end'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    437\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    438\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mon_test_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/callbacks.py\u001b[0m in \u001b[0;36m_call_batch_hook\u001b[0;34m(self, mode, hook, batch, logs)\u001b[0m\n\u001b[1;32m    293\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_batch_begin_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    294\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'end'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 295\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_batch_end_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    296\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    297\u001b[0m       raise ValueError(\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/callbacks.py\u001b[0m in \u001b[0;36m_call_batch_end_hook\u001b[0;34m(self, mode, batch, logs)\u001b[0m\n\u001b[1;32m    314\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_batch_times\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_time\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    315\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 316\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_batch_hook_helper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhook_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    317\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    318\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_batch_times\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_batches_for_timing_check\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/callbacks.py\u001b[0m in \u001b[0;36m_call_batch_hook_helper\u001b[0;34m(self, hook_name, batch, logs)\u001b[0m\n\u001b[1;32m    352\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mcallback\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    353\u001b[0m       \u001b[0mhook\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcallback\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 354\u001b[0;31m       \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    355\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    356\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_timing\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/callbacks.py\u001b[0m in \u001b[0;36mon_train_batch_end\u001b[0;34m(self, batch, logs)\u001b[0m\n\u001b[1;32m   1030\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1031\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mon_train_batch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1032\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_batch_update_progbar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1033\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1034\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mon_test_batch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/callbacks.py\u001b[0m in \u001b[0;36m_batch_update_progbar\u001b[0;34m(self, batch, logs)\u001b[0m\n\u001b[1;32m   1102\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mverbose\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1103\u001b[0m       \u001b[0;31m# Only block async when verbose = 1.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1104\u001b[0;31m       \u001b[0mlogs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msync_to_numpy_or_python_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1105\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprogbar\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfinalize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1106\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/utils/tf_utils.py\u001b[0m in \u001b[0;36msync_to_numpy_or_python_type\u001b[0;34m(tensors)\u001b[0m\n\u001b[1;32m    552\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mt\u001b[0m  \u001b[0;31m# Don't turn ragged or sparse tensors to NumPy.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    553\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 554\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap_structure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_to_single_numpy_or_python_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    555\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    556\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/util/nest.py\u001b[0m in \u001b[0;36mmap_structure\u001b[0;34m(func, *structure, **kwargs)\u001b[0m\n\u001b[1;32m    867\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    868\u001b[0m   return pack_sequence_as(\n\u001b[0;32m--> 869\u001b[0;31m       \u001b[0mstructure\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mentries\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    870\u001b[0m       expand_composites=expand_composites)\n\u001b[1;32m    871\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/util/nest.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    867\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    868\u001b[0m   return pack_sequence_as(\n\u001b[0;32m--> 869\u001b[0;31m       \u001b[0mstructure\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mentries\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    870\u001b[0m       expand_composites=expand_composites)\n\u001b[1;32m    871\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/utils/tf_utils.py\u001b[0m in \u001b[0;36m_to_single_numpy_or_python_type\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m    548\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_to_single_numpy_or_python_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m       \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    551\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mt\u001b[0m  \u001b[0;31m# Don't turn ragged or sparse tensors to NumPy.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mnumpy\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1147\u001b[0m     \"\"\"\n\u001b[1;32m   1148\u001b[0m     \u001b[0;31m# TODO(slebedev): Consider avoiding a copy for non-CPU or remote tensors.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1149\u001b[0;31m     \u001b[0mmaybe_arr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1150\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mmaybe_arr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmaybe_arr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mmaybe_arr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1151\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36m_numpy\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1113\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1114\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1115\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_numpy_internal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1116\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1117\u001b[0m       \u001b[0;32mraise\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_status_to_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T35K66bzTArf"
      },
      "source": [
        "## Results\n",
        "\n",
        "This model has pretty normal graphs except for accuracy, which confuses me so much. Please take a look and see if you can find anything wrong with the data formatting or training, as the accuraccy *does* improve over epochs, but starts out at near 0. Precision and Recall are also suspiciously high, at 0.99+ each. \n",
        "\n",
        "It's possible this is simply because I used the wrong metrics, as the model seems to be pretty accurate for individual files from the training/validation set (see below)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DsTOkacZM7Qi"
      },
      "source": [
        "from matplotlib import pyplot as plt\n",
        "def plot(data, labels, x, y):\n",
        "    '''Plot statistics. Takes a list of lists and list of labels.'''\n",
        "    \n",
        "    print(y + \" vs. \" + x)\n",
        "    for i in range(len(data)):\n",
        "        plt.plot(data[i], label=labels[i])\n",
        "    plt.xlabel(x)\n",
        "    plt.ylabel(y)\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "    print()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5eodNFe6M7Qj"
      },
      "source": [
        "plot((history.history[\"loss\"], ), (\"Loss\", ), \"Epoch\", \"Loss\")\n",
        "plot((history.history[\"accuracy\"], history.history[\"precision\"], history.history[\"recall\"], ), (\"Accuracy\", \"Precision\", \"Recall\", ), \"Epoch\", \"Accuracy, Precision, Recall\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5kdaOzg3srSD"
      },
      "source": [
        "## Testing for individual sound files\n",
        "\n",
        "The model seems to be pretty good for the first couple of seconds, but then becomes very inaccurate as the pitch fades."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qdMWs4vsM7Qk"
      },
      "source": [
        "import soundfile\n",
        "def load_audio(file):\n",
        "    wav, sr = soundfile.read(file)\n",
        "    return wav, sr"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Doqk900asdr_"
      },
      "source": [
        "from numpy.lib.stride_tricks import as_strided\n",
        "def predict(audio, sr, step_size=10):\n",
        "    if len(audio.shape) == 2:\n",
        "        audio = audio.mean(1)  # make mono\n",
        "    audio = audio.astype(np.float32)\n",
        "    if sr != model_srate:\n",
        "        # resample audio if necessary\n",
        "        from resampy import resample\n",
        "        audio = resample(audio, sr, model_srate)\n",
        "\n",
        "    # make 1024-sample frames of the audio with hop length of 10 milliseconds\n",
        "    hop_length = int(model_srate * step_size / 1000)\n",
        "    n_frames = 1 + int((len(audio) - 1024) / hop_length)\n",
        "    frames = as_strided(audio, shape=(1024, n_frames),\n",
        "                        strides=(audio.itemsize, hop_length * audio.itemsize))\n",
        "    frames = frames.transpose().copy()\n",
        "\n",
        "    # normalize each frame -- this is expected by the model\n",
        "    frames -= np.mean(frames, axis=1)[:, np.newaxis]\n",
        "    frames /= np.std(frames, axis=1)[:, np.newaxis]\n",
        "\n",
        "    # run prediction and convert the frequency bin weights to Hz\n",
        "    return model(frames)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ojdkYbBisfIP"
      },
      "source": [
        "def as_midi(pred):\n",
        "    # Convert from output buckets back to MIDI\n",
        "    midi = (pred.argmax(axis=1) / 5) + 24\n",
        "    return midi"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "26CgjLWCtHKw"
      },
      "source": [
        "## Steelpan-trained CREPE vs Original CREPE"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NaXp0A9Dsh1Q",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "75070a16-bec3-401a-94ed-b8028f79d32b"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "model = make_model(\"full\", [], '/content/test-crepe-full_colab_10.h5')\n",
        "wav, sr = load_audio('/content/drive/MyDrive/Research Projects/Steelpan pitch detection/Jason\\'s Work/tiny_16kHz/validation/68_train_sample_135.wav')\n",
        "pred = as_midi(predict(wav, sr).numpy())\n",
        "print(pred)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[68.  68.  68.  68.  68.  80.  80.  80.  80.  68.  68.  68.  68.  68.\n",
            " 68.  68.  68.  68.  68.  68.  68.  68.  68.  68.  68.  68.  68.  68.\n",
            " 68.  68.  68.  68.  68.  68.  68.  68.  68.  68.  68.  68.  68.  68.\n",
            " 68.  68.  68.  68.  68.  68.  68.  68.  68.  68.  68.  68.  68.  68.\n",
            " 68.  68.  68.  68.  68.  68.  68.  68.  68.  68.  68.  68.  68.  68.\n",
            " 68.  68.  68.  68.  68.  68.  68.  68.  68.  68.  68.  68.  68.  68.\n",
            " 68.  68.  68.  68.  68.  68.  68.  68.  68.  68.  68.  68.  68.  68.\n",
            " 68.  68.  68.  68.  68.  68.  68.  68.  68.  68.  68.  68.  68.  68.\n",
            " 68.  68.  68.  68.  68.  68.  68.  68.  68.  68.  68.  68.  68.  68.\n",
            " 68.  68.  68.  68.  68.  68.  68.  68.  68.  68.  68.  68.  68.  68.\n",
            " 68.  68.  68.  68.  68.  68.  68.  68.  68.  68.  68.  68.  68.  68.\n",
            " 68.  68.  68.  68.  68.  68.  68.  68.  68.  68.  68.  68.  68.  68.\n",
            " 68.  68.  68.  68.  68.  68.  68.  68.  68.  68.  68.  68.  68.  68.\n",
            " 68.  68.  68.  68.  68.  68.  68.  68.  68.  68.  68.  53.  53.  53.\n",
            " 50.8 51.6 68.  68.  68.  68.  68.  68.  68.  47.8 55.8 55.8 68.  53.\n",
            " 53.  53.  53.  52.8 52.8 52.8 52.8 51.6 50.8 51.8 51.6 53.  53.2 53.\n",
            " 54.8 54.8 54.8 44.4 55.8 53.  53.2 54.8 53.  53.  53.  53.  53.4 53.2\n",
            " 53.  53.  47.8 47.8 47.8 47.8 47.2 47.2 47.2 47.8 54.6 53.2 53.  53.\n",
            " 53.  50.8 50.8 70.  55.8 54.8 47.8 53.  53.  53.  53.  53.  53.2 47.2\n",
            " 52.8 51.8 44.6 44.6 45.2 57.4 56.8 57.2 56.8 54.8 53. ]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8YBzll2i7otf"
      },
      "source": [
        "import IPython.display as ipd"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AhYkNUhs7otg"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}