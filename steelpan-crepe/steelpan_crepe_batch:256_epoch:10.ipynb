{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "steelpan-crepe.ipynb.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "interpreter": {
      "hash": "e92005438139a4419431294f52efbb345a0cf152313ea7bfec8e749b14196e5a"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/malloyca/steelpan-pitch/blob/main/steelpan-crepe/steelpan_crepe_batch%3A256_epoch%3A10.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T7gdvjLll-3o"
      },
      "source": [
        "# Steelpan-crepe Training Notebook"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XZ5vVsiSNaUN"
      },
      "source": [
        "This notebook can create CREPE models and train them on steelpan data.\n",
        "\n",
        "To use the notebook as is, download the downsampled audio (\"tiny_16kHz/\") and make sure that the dirpaths for the train and validation sets are correct. Also download the .h5 files from the CREPE repo's \"models\" branch if you want to train from existing weights."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GNU0hEyjM7QZ",
        "outputId": "555d3f07-bd02-49a4-a24b-1aedfdc77dee"
      },
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import os\n",
        "import soundfile\n",
        "import librosa # Is this needed after all?\n",
        "\n",
        "print(tf.__version__)\n",
        "# This code allows for the GPU to be utilized properly.\n",
        "tf.autograph.set_verbosity(0)\n",
        "physical_devices = tf.config.list_physical_devices(\"GPU\")\n",
        "try:\n",
        "    tf.config.experimental.set_memory_growth(physical_devices[0], True)\n",
        "except:\n",
        "    pass\n",
        "\n",
        "print(physical_devices)\n",
        "print(\"If the above list is empty, then TF won't use any accelerator\")"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.6.0\n",
            "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n",
            "If the above list is empty, then TF won't use any accelerator\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vWCnYvL68l_k",
        "outputId": "8aa82796-7436-4d08-d9f4-84d0905ab9c0"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Y-2mq23uCcs"
      },
      "source": [
        "# the model is trained on 16kHz audio\n",
        "model_srate = 16000\n",
        "\n",
        "# set batch size for training\n",
        "batch_size = 256"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0icYDMG8PZ1I"
      },
      "source": [
        "## Model builder\n",
        "\n",
        "This code is modified a bit from the repo as it normally stores the models in a dict, which I think is unnecessary for our purposes.\n",
        "\n",
        "You can also load weights from an existing .h5 file. I began training by loading the weights of \"model-full.h5\" from the marl/crepe models branch. The weights for the model trained on steelpan data is named \"new-crepe-full.h5\" and is in the Drive folder."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pamdqnF5M7Qe"
      },
      "source": [
        "# todo - This is a note from Crepe, delete and set it up to just do a 'full' model\n",
        "# store as a global variable, since we only support a few models for now\n",
        "models = {\n",
        "    'tiny': None,\n",
        "    'small': None,\n",
        "    'medium': None,\n",
        "    'large': None,\n",
        "    'full': None\n",
        "}\n",
        "\n",
        "def make_model(model_capacity, metrics, weights=None):\n",
        "    '''\n",
        "    model_capacity: tiny, small, medium, large, full\n",
        "    weights: path of .h5 weights file\n",
        "    '''\n",
        "\n",
        "    from tensorflow.keras.layers import Input, Reshape, Conv2D, BatchNormalization\n",
        "    from tensorflow.keras.layers import MaxPool2D, Dropout, Permute, Flatten, Dense\n",
        "    from tensorflow.keras.models import Model\n",
        "\n",
        "    capacity_multiplier = {\n",
        "        'tiny': 4, 'small': 8, 'medium': 16, 'large': 24, 'full': 32\n",
        "    }[model_capacity]\n",
        "\n",
        "    layers = [1, 2, 3, 4, 5, 6]\n",
        "    filters = [n * capacity_multiplier for n in [32, 4, 4, 4, 8, 16]]\n",
        "    widths = [512, 64, 64, 64, 64, 64]\n",
        "    strides = [(4, 1), (1, 1), (1, 1), (1, 1), (1, 1), (1, 1)]\n",
        "\n",
        "    x = Input(shape=(1024,), name='input', dtype='float32')\n",
        "    y = Reshape(target_shape=(1024, 1, 1), name='input-reshape')(x)\n",
        "\n",
        "    for l, f, w, s in zip(layers, filters, widths, strides):\n",
        "        y = Conv2D(f, (w, 1), strides=s, padding='same',\n",
        "                    activation='relu', name=\"conv%d\" % l)(y)\n",
        "        y = BatchNormalization(name=\"conv%d-BN\" % l)(y)\n",
        "        y = MaxPool2D(pool_size=(2, 1), strides=None, padding='valid',\n",
        "                        name=\"conv%d-maxpool\" % l)(y)\n",
        "        y = Dropout(0.25, name=\"conv%d-dropout\" % l)(y)\n",
        "\n",
        "    y = Permute((2, 1, 3), name=\"transpose\")(y)\n",
        "    y = Flatten(name=\"flatten\")(y)\n",
        "    y = Dense(360, activation='sigmoid', name=\"classifier\")(y)\n",
        "\n",
        "    model = Model(inputs=x, outputs=y)\n",
        "\n",
        "    if weights != None:\n",
        "        model.load_weights(weights)\n",
        "    model.compile(tf.keras.optimizers.Adam(learning_rate=0.0002), 'binary_crossentropy', metrics=metrics)\n",
        "\n",
        "    models[model_capacity] = model\n",
        "\n",
        "    return model"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SIx7SuJTnzD8"
      },
      "source": [
        "# todo - Just use Librosa's db_to_power instead?\n",
        "def db_to_pow(db):\n",
        "  '''Convert from dB to power'''\n",
        "  return 10**(db / 10)\n",
        "\n",
        "\n",
        "def frame_energy(frame):\n",
        "  '''Calculates the average energy for a frame\n",
        "    \n",
        "    Parameters\n",
        "    ----------\n",
        "    frame : np.array\n",
        "      audio frame in np.float32 format\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    average_energy : float\n",
        "      Average energy level for frame\n",
        "  '''\n",
        "\n",
        "  # Square the sample values to convert to energy values\n",
        "  energy = frame**2\n",
        "\n",
        "  # Sum the energy values to get total energy\n",
        "  total_energy = np.sum(energy)\n",
        "\n",
        "  # Divide by length to get average energy\n",
        "  return total_energy / len(frame)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LUIdeIZOQoTJ"
      },
      "source": [
        "## Data formatting\n",
        "\n",
        "Audio is formatted according to how CREPE does it, but is formatted into batch format for input into model.fit(). Step size defaults to 10ms like in CREPE.\n",
        "\n",
        "The label data is scaled and encoded into one-hots to fit the model's bucketed output. There are 360 outputs, but only 29 actually appear as labels (MIDI pitches 60-89).\n",
        "\n",
        "Data is being trimmed by dBFS levels, which I've set to -30 for now. Anything -40 and below trims almost nothing, and -30 trims a good amount (I think it does need more tuning). **No, trimming needs to happen before normalization. That's the issue.**\n",
        "\n",
        "The one-hotted labels have Gaussian blurring applied to them too (as specified in the other notebook)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UMdS1JKxM7Qf"
      },
      "source": [
        "# todo - fix this\n",
        "# The latest changes have broken the dimensions of the output frames (1024,)\n",
        "# The labels appear accurate, but check it\n",
        "\n",
        "# todo - Can we re-code this using Numba to speed it up?\n",
        "\n",
        "from os import walk\n",
        "\n",
        "def load_audio_batch(dir, model_srate=16000, threshold_db=-60, step_size=10, blur=True):\n",
        "    '''Load a batch of audio files \n",
        "    \n",
        "    Parameters\n",
        "    ----------\n",
        "    dir : str\n",
        "        Filepath of directory containing audio files to load\n",
        "    threshold_db : int\n",
        "        Threshold for cutting leading and trailing silence of audio files in dB\n",
        "    step_size : float\n",
        "        Step size between audio frames in ms\n",
        "    \n",
        "    Returns\n",
        "    ------\n",
        "    audio_frames : np.ndarray[shape=(1024,360,#todo)]\n",
        "    labels : np.ndarray[shape=(360,)]\n",
        "    '''\n",
        "    \n",
        "    from numpy.lib.stride_tricks import as_strided\n",
        "    \n",
        "    # Initialize arrays\n",
        "    audio_frames = np.empty((0,1024), dtype=np.float32)\n",
        "    audio_labels = []\n",
        "\n",
        "    hop_length = int(model_srate * step_size / 1000)\n",
        "    threshold_pow = db_to_pow(threshold_db)\n",
        "\n",
        "    #test\n",
        "    count = 0\n",
        "    # Iterate over files in directory\n",
        "    for (dirpath, dirnames, filenames) in os.walk(dir):\n",
        "        for filename in filenames:\n",
        "            # load audio\n",
        "            audio, _ = soundfile.read(dirpath + \"/\" + filename)\n",
        "            \n",
        "            \n",
        "            # Split into audio frames\n",
        "            n_frames = 1 + int((len(audio) - 1024) / hop_length)\n",
        "            framed_audio = as_strided(audio, shape=(1024, n_frames),\n",
        "                                strides=(audio.itemsize, hop_length * audio.itemsize))\n",
        "            framed_audio = framed_audio.transpose().copy()\n",
        "            \n",
        "            \n",
        "             # Trim audio leading and trailing silence from audio\n",
        "            for f in range(len(framed_audio)):\n",
        "                if frame_energy(framed_audio[f]) > threshold_pow:\n",
        "                    start_frame = f\n",
        "                    break\n",
        "\n",
        "            for f in range(len(framed_audio) - 1, -1, -1):\n",
        "                if frame_energy(framed_audio[f]) > threshold_pow:\n",
        "                    end_frame = f + 1\n",
        "                    break\n",
        "                    \n",
        "            trimmed_audio = framed_audio[start_frame:end_frame]\n",
        "\n",
        "            \n",
        "            # Normalize the audio data by frame\n",
        "            trimmed_audio -= np.mean(trimmed_audio, axis=1)[:, np.newaxis]\n",
        "            trimmed_audio /= np.std(trimmed_audio, axis=1)[:, np.newaxis]\n",
        "            \n",
        "            # Append normalized audio to audio_frames array\n",
        "            audio_frames = np.append(audio_frames, trimmed_audio, axis=0)\n",
        "            \n",
        "            \n",
        "            \n",
        "            # Append values to the labels array\n",
        "            audio_labels += [int(filename.split(\"_\")[0]) for _ in range(end_frame - start_frame)]\n",
        "            \n",
        "            \n",
        "    \n",
        "    # Convert audio_labels to numpy array\n",
        "    audio_labels = np.array(tf.one_hot(5 * (np.array(audio_labels) - 24), 360))\n",
        "    \n",
        "    if blur:\n",
        "        # Apply Gaussian blur to labels\n",
        "        cents_i = np.arange(360)\n",
        "        for i in range(len(audio_labels)):\n",
        "            cents_true = np.where(audio_labels[i] == 1)[0][0]\n",
        "            audio_labels[i] = np.exp(-((20 *(cents_i - cents_true)) ** 2) / (2 * (25 ** 2)))\n",
        "\n",
        "        \n",
        "    return audio_frames, audio_labels"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r2zhlSGiM7Qg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8557dba8-e272-42bb-e3bf-df6336796a80"
      },
      "source": [
        "metrics = \"Accuracy\"\n",
        "model = make_model(\"full\", metrics=metrics, weights='/content/drive/MyDrive/Research Projects/Steelpan pitch detection/Jason\\'s Work/model-full.h5')\n",
        "model.summary()"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_7\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input (InputLayer)           [(None, 1024)]            0         \n",
            "_________________________________________________________________\n",
            "input-reshape (Reshape)      (None, 1024, 1, 1)        0         \n",
            "_________________________________________________________________\n",
            "conv1 (Conv2D)               (None, 256, 1, 1024)      525312    \n",
            "_________________________________________________________________\n",
            "conv1-BN (BatchNormalization (None, 256, 1, 1024)      4096      \n",
            "_________________________________________________________________\n",
            "conv1-maxpool (MaxPooling2D) (None, 128, 1, 1024)      0         \n",
            "_________________________________________________________________\n",
            "conv1-dropout (Dropout)      (None, 128, 1, 1024)      0         \n",
            "_________________________________________________________________\n",
            "conv2 (Conv2D)               (None, 128, 1, 128)       8388736   \n",
            "_________________________________________________________________\n",
            "conv2-BN (BatchNormalization (None, 128, 1, 128)       512       \n",
            "_________________________________________________________________\n",
            "conv2-maxpool (MaxPooling2D) (None, 64, 1, 128)        0         \n",
            "_________________________________________________________________\n",
            "conv2-dropout (Dropout)      (None, 64, 1, 128)        0         \n",
            "_________________________________________________________________\n",
            "conv3 (Conv2D)               (None, 64, 1, 128)        1048704   \n",
            "_________________________________________________________________\n",
            "conv3-BN (BatchNormalization (None, 64, 1, 128)        512       \n",
            "_________________________________________________________________\n",
            "conv3-maxpool (MaxPooling2D) (None, 32, 1, 128)        0         \n",
            "_________________________________________________________________\n",
            "conv3-dropout (Dropout)      (None, 32, 1, 128)        0         \n",
            "_________________________________________________________________\n",
            "conv4 (Conv2D)               (None, 32, 1, 128)        1048704   \n",
            "_________________________________________________________________\n",
            "conv4-BN (BatchNormalization (None, 32, 1, 128)        512       \n",
            "_________________________________________________________________\n",
            "conv4-maxpool (MaxPooling2D) (None, 16, 1, 128)        0         \n",
            "_________________________________________________________________\n",
            "conv4-dropout (Dropout)      (None, 16, 1, 128)        0         \n",
            "_________________________________________________________________\n",
            "conv5 (Conv2D)               (None, 16, 1, 256)        2097408   \n",
            "_________________________________________________________________\n",
            "conv5-BN (BatchNormalization (None, 16, 1, 256)        1024      \n",
            "_________________________________________________________________\n",
            "conv5-maxpool (MaxPooling2D) (None, 8, 1, 256)         0         \n",
            "_________________________________________________________________\n",
            "conv5-dropout (Dropout)      (None, 8, 1, 256)         0         \n",
            "_________________________________________________________________\n",
            "conv6 (Conv2D)               (None, 8, 1, 512)         8389120   \n",
            "_________________________________________________________________\n",
            "conv6-BN (BatchNormalization (None, 8, 1, 512)         2048      \n",
            "_________________________________________________________________\n",
            "conv6-maxpool (MaxPooling2D) (None, 4, 1, 512)         0         \n",
            "_________________________________________________________________\n",
            "conv6-dropout (Dropout)      (None, 4, 1, 512)         0         \n",
            "_________________________________________________________________\n",
            "transpose (Permute)          (None, 1, 4, 512)         0         \n",
            "_________________________________________________________________\n",
            "flatten (Flatten)            (None, 2048)              0         \n",
            "_________________________________________________________________\n",
            "classifier (Dense)           (None, 360)               737640    \n",
            "=================================================================\n",
            "Total params: 22,244,328\n",
            "Trainable params: 22,239,976\n",
            "Non-trainable params: 4,352\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uP4v_bqtSMWe"
      },
      "source": [
        "## Loading/Formatting data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8M3ZoJIiM7Qh"
      },
      "source": [
        "x_train, y_train = load_audio_batch('/content/drive/MyDrive/Research Projects/Steelpan pitch detection/Jason\\'s Work/tiny_16kHz/train', blur=True)\n",
        "x_val, y_val = load_audio_batch('/content/drive/MyDrive/Research Projects/Steelpan pitch detection/Jason\\'s Work/tiny_16kHz/validation', blur=False)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bbCYJaC4Sio2"
      },
      "source": [
        "## Training\n",
        "\n",
        "Any batch size larger than 256 is slower, and also has trouble fitting into VRAM. 20 epochs was an arbitrary choice, which took roughly an hour and a half on a 1650 super.\n",
        "\n",
        "This also saves the model's weights after training."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gjw9yuKbnGmr"
      },
      "source": [
        "# todo - Do I need to include comparator instructions (minimize)? Auto mode seems to be making the wrong choice\n",
        "early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_Accuracy', patience=32, mode='min')\n",
        "# todo - check to make sure this is correct - I think the first time I tried this, the checkpoint callback failed because I didn't capitalize the A\n",
        "# todo - Set save_weights_only to False? I want to save the whole model.\n",
        "checkpoints = tf.keras.callbacks.ModelCheckpoint(filepath='/retrain_weights_colab_batch_size:256_epoch:{epoch:02d}_val_Accuracy:{val_Accuracy:.2f}.hdf5',\n",
        "                                                 monitor='val_Accuracy',\n",
        "                                                 mode='max',\n",
        "                                                 save_best_only=True)\n",
        "\n",
        "callbacks = [early_stopping, checkpoints]"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jyDFC64UM7Qi",
        "pycharm": {
          "is_executing": true
        },
        "scrolled": false,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fd7a9f45-eacf-43a5-cb55-d24383b960c5"
      },
      "source": [
        "# todo - Increasing the batch size slows convergence. The original model was trained on 32 sample batches. Is there difference in performance depending on the batch size?\n",
        "# todo - enable callbacks... history = model.fit(..., callbacks=callbacks, ...)\n",
        "# Batch size will effect batch normalization, backpropagation, etc...\n",
        "# Anecdotally, it seemed that I got better validation accuracy scores using batch_size=128\n",
        "history = model.fit(x=x_train, y=y_train, batch_size=batch_size, epochs=10,\n",
        "                    validation_data=(x_val, y_val),\n",
        "                    callbacks=callbacks)\n",
        "\n",
        "# This shouldn't be necessary anymore since checkpoints is set up\n",
        "#model.save('test-crepe-full_colab_batch:256_epoch:10.h5')"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "303/303 [==============================] - 270s 884ms/step - loss: 0.0230 - Accuracy: 0.2216 - val_loss: 0.0108 - val_Accuracy: 0.8268\n",
            "Epoch 2/10\n",
            "303/303 [==============================] - 267s 883ms/step - loss: 0.0116 - Accuracy: 0.9195 - val_loss: 0.0116 - val_Accuracy: 0.9435\n",
            "Epoch 3/10\n",
            "303/303 [==============================] - 267s 883ms/step - loss: 0.0099 - Accuracy: 0.9708 - val_loss: 0.0112 - val_Accuracy: 0.9606\n",
            "Epoch 4/10\n",
            "303/303 [==============================] - 268s 884ms/step - loss: 0.0092 - Accuracy: 0.9849 - val_loss: 0.0111 - val_Accuracy: 0.9692\n",
            "Epoch 5/10\n",
            "303/303 [==============================] - 267s 882ms/step - loss: 0.0089 - Accuracy: 0.9901 - val_loss: 0.0112 - val_Accuracy: 0.9717\n",
            "Epoch 6/10\n",
            "303/303 [==============================] - 268s 884ms/step - loss: 0.0087 - Accuracy: 0.9936 - val_loss: 0.0111 - val_Accuracy: 0.9712\n",
            "Epoch 7/10\n",
            "303/303 [==============================] - 268s 883ms/step - loss: 0.0086 - Accuracy: 0.9955 - val_loss: 0.0109 - val_Accuracy: 0.9731\n",
            "Epoch 8/10\n",
            "303/303 [==============================] - 268s 883ms/step - loss: 0.0085 - Accuracy: 0.9960 - val_loss: 0.0107 - val_Accuracy: 0.9737\n",
            "Epoch 9/10\n",
            "303/303 [==============================] - 268s 884ms/step - loss: 0.0084 - Accuracy: 0.9971 - val_loss: 0.0109 - val_Accuracy: 0.9736\n",
            "Epoch 10/10\n",
            "303/303 [==============================] - 268s 884ms/step - loss: 0.0083 - Accuracy: 0.9978 - val_loss: 0.0106 - val_Accuracy: 0.9745\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T35K66bzTArf"
      },
      "source": [
        "## Results\n",
        "\n",
        "This model has pretty normal graphs except for accuracy, which confuses me so much. Please take a look and see if you can find anything wrong with the data formatting or training, as the accuraccy *does* improve over epochs, but starts out at near 0. Precision and Recall are also suspiciously high, at 0.99+ each. \n",
        "\n",
        "It's possible this is simply because I used the wrong metrics, as the model seems to be pretty accurate for individual files from the training/validation set (see below)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DsTOkacZM7Qi"
      },
      "source": [
        "from matplotlib import pyplot as plt\n",
        "def plot(data, labels, x, y):\n",
        "    '''Plot statistics. Takes a list of lists and list of labels.'''\n",
        "    \n",
        "    print(y + \" vs. \" + x)\n",
        "    for i in range(len(data)):\n",
        "        plt.plot(data[i], label=labels[i])\n",
        "    plt.xlabel(x)\n",
        "    plt.ylabel(y)\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "    print()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5eodNFe6M7Qj"
      },
      "source": [
        "plot((history.history[\"loss\"], ), (\"Loss\", ), \"Epoch\", \"Loss\")\n",
        "plot((history.history[\"accuracy\"], history.history[\"precision\"], history.history[\"recall\"], ), (\"Accuracy\", \"Precision\", \"Recall\", ), \"Epoch\", \"Accuracy, Precision, Recall\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5kdaOzg3srSD"
      },
      "source": [
        "## Testing for individual sound files\n",
        "\n",
        "The model seems to be pretty good for the first couple of seconds, but then becomes very inaccurate as the pitch fades."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qdMWs4vsM7Qk"
      },
      "source": [
        "import soundfile\n",
        "def load_audio(file):\n",
        "    wav, sr = soundfile.read(file)\n",
        "    return wav, sr"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Doqk900asdr_"
      },
      "source": [
        "from numpy.lib.stride_tricks import as_strided\n",
        "def predict(audio, sr, step_size=10):\n",
        "    if len(audio.shape) == 2:\n",
        "        audio = audio.mean(1)  # make mono\n",
        "    audio = audio.astype(np.float32)\n",
        "    if sr != model_srate:\n",
        "        # resample audio if necessary\n",
        "        from resampy import resample\n",
        "        audio = resample(audio, sr, model_srate)\n",
        "\n",
        "    # make 1024-sample frames of the audio with hop length of 10 milliseconds\n",
        "    hop_length = int(model_srate * step_size / 1000)\n",
        "    n_frames = 1 + int((len(audio) - 1024) / hop_length)\n",
        "    frames = as_strided(audio, shape=(1024, n_frames),\n",
        "                        strides=(audio.itemsize, hop_length * audio.itemsize))\n",
        "    frames = frames.transpose().copy()\n",
        "\n",
        "    # normalize each frame -- this is expected by the model\n",
        "    frames -= np.mean(frames, axis=1)[:, np.newaxis]\n",
        "    frames /= np.std(frames, axis=1)[:, np.newaxis]\n",
        "\n",
        "    # run prediction and convert the frequency bin weights to Hz\n",
        "    return model(frames)"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ojdkYbBisfIP"
      },
      "source": [
        "def as_midi(pred):\n",
        "    # Convert from output buckets back to MIDI\n",
        "    midi = (pred.argmax(axis=1) / 5) + 24\n",
        "    return midi"
      ],
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "26CgjLWCtHKw"
      },
      "source": [
        "## Steelpan-trained CREPE vs Original CREPE"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NaXp0A9Dsh1Q",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f0da36ab-fa44-4bc8-dc43-a6f4bab08f84"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "model = make_model(\"full\", [], '/retrain_weights_colab_batch_size:256_epoch:10_val_Accuracy:0.97.hdf5')\n",
        "wav, sr = load_audio('/content/drive/MyDrive/Research Projects/Steelpan pitch detection/Jason\\'s Work/tiny_16kHz/validation/68_train_sample_135.wav')\n",
        "pred = as_midi(predict(wav, sr).numpy())\n",
        "print(pred)"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[68.  68.  68.  68.  80.  80.  80.  80.  80.  68.  68.  68.  68.  68.\n",
            " 68.  68.  68.  68.  68.  68.  68.  68.  68.  68.  68.  68.  68.  68.\n",
            " 68.  68.  68.  68.  68.  68.  68.  68.  68.  68.  68.  68.  68.  68.\n",
            " 68.  68.  68.  68.  68.  68.  68.  68.  68.  68.  68.  68.  68.  68.\n",
            " 68.  68.  68.  68.  68.  68.  68.  68.  68.  68.  68.  68.  68.  68.\n",
            " 68.  68.  68.  68.  68.  68.  68.  68.  68.  68.  68.  68.  68.  68.\n",
            " 68.  68.  68.  68.  68.  68.  68.  68.  68.  68.  68.  68.  68.  68.\n",
            " 68.  68.  68.  68.  68.  68.  68.  68.  68.  68.  68.  68.  68.  68.\n",
            " 68.  68.  68.  68.  68.  68.  68.  68.  68.  68.  68.  68.  68.  68.\n",
            " 68.  68.  68.  68.  68.  68.  68.  68.  68.  68.  68.  68.  68.  68.\n",
            " 68.  68.  68.  68.  68.  68.  68.  68.  68.  68.  68.  68.  68.  68.\n",
            " 68.  68.  68.  68.  68.  68.  68.  68.  68.  68.  68.  68.  68.  68.\n",
            " 68.  68.  68.  68.  68.  68.  68.  68.  68.  68.  68.  68.  68.  68.\n",
            " 68.  68.  68.  68.  68.  68.  68.  68.  68.  68.  68.  53.  53.  53.\n",
            " 53.  51.6 68.  68.  68.  68.  68.  68.  68.  47.8 55.8 55.8 68.  53.\n",
            " 53.  53.  53.  52.8 52.8 52.8 52.8 51.6 50.8 51.8 51.6 53.  53.2 53.\n",
            " 54.8 54.8 54.8 51.6 53.  53.  53.2 53.2 53.  53.  53.  53.  53.  53.2\n",
            " 53.  53.  47.8 47.8 47.8 47.8 47.2 47.2 47.2 47.8 54.6 53.2 53.  53.\n",
            " 53.  50.8 50.8 70.  55.8 54.8 47.8 53.  53.  53.  53.  53.  53.  52.6\n",
            " 52.8 51.8 51.6 44.8 57.2 57.4 56.8 57.2 56.8 53.  53. ]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8YBzll2i7otf"
      },
      "source": [
        "import IPython.display as ipd"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AhYkNUhs7otg"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}