{
  "nbformat": 4,
  "nbformat_minor": 2,
  "metadata": {
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    },
    "orig_nbformat": 4,
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3.8.8 64-bit ('tf': conda)"
    },
    "interpreter": {
      "hash": "e92005438139a4419431294f52efbb345a0cf152313ea7bfec8e749b14196e5a"
    },
    "colab": {
      "name": "Copy of CREPE_Trainer.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Changes in this version\r\n",
        "* Optimize loading & processing audio for speed\r\n",
        "* Split loading audio into 2 functions"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "# CREPE Trainer\n",
        "\n",
        "This notebook can create CREPE models and train them on steelpan data.\n",
        "\n",
        "To use the notebook as is, download the downsampled audio (\"tiny_16kHz/\") and make sure that the dirpaths for the train and validation sets are correct. Also download the .h5 files from the CREPE repo's \"models\" branch if you want to train from existing weights."
      ],
      "metadata": {
        "id": "XZ5vVsiSNaUN"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "source": [
        "import tensorflow as tf\r\n",
        "import numpy as np\r\n",
        "\r\n",
        "print(tf.__version__)\r\n",
        "# This code allows for the GPU to be utilized properly.\r\n",
        "tf.autograph.set_verbosity(0)\r\n",
        "physical_devices = tf.config.list_physical_devices(\"GPU\")\r\n",
        "try:\r\n",
        "    tf.config.experimental.set_memory_growth(physical_devices[0], True)\r\n",
        "except:\r\n",
        "    pass\r\n",
        "\r\n",
        "print(physical_devices)\r\n",
        "print(\"If the above list is empty, then TF won't use any accelerator\")"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.3.0\n",
            "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n",
            "If the above list is empty, then TF won't use any accelerator\n"
          ]
        }
      ],
      "metadata": {
        "id": "GNU0hEyjM7QZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9f42496f-9cdd-43c7-ce58-414972f00f60"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model builder\r\n",
        "\r\n",
        "This code is modified a bit from the repo as it normally stores the models in a dict, which I think is unnecessary for our purposes.\r\n",
        "\r\n",
        "You can also load weights from an existing .h5 file. I began training by loading the weights of \"model-full.h5\" from the marl/crepe models branch. The weights for the model trained on steelpan data is named \"new-crepe-full.h5\" and is in the Drive folder."
      ],
      "metadata": {
        "id": "0icYDMG8PZ1I"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "source": [
        "import os\r\n",
        "\r\n",
        "# store as a global variable, since we only support a few models for now\r\n",
        "models = {\r\n",
        "    'tiny': None,\r\n",
        "    'small': None,\r\n",
        "    'medium': None,\r\n",
        "    'large': None,\r\n",
        "    'full': None\r\n",
        "}\r\n",
        "\r\n",
        "# the model is trained on 16kHz audio\r\n",
        "model_srate = 16000\r\n",
        "model_size = \"medium\"\r\n",
        "\r\n",
        "def make_model(model_capacity, metrics, weights=None):\r\n",
        "    '''\r\n",
        "    model_capacity: tiny, small, medium, large, full\r\n",
        "    weights: path of .h5 weights file\r\n",
        "    '''\r\n",
        "\r\n",
        "    from tensorflow.keras.layers import Input, Reshape, Conv2D, BatchNormalization\r\n",
        "    from tensorflow.keras.layers import MaxPool2D, Dropout, Permute, Flatten, Dense\r\n",
        "    from tensorflow.keras.models import Model\r\n",
        "\r\n",
        "    capacity_multiplier = {\r\n",
        "        'tiny': 4, 'small': 8, 'medium': 16, 'large': 24, 'full': 32\r\n",
        "    }[model_capacity]\r\n",
        "\r\n",
        "    layers = [1, 2, 3, 4, 5, 6]\r\n",
        "    filters = [n * capacity_multiplier for n in [32, 4, 4, 4, 8, 16]]\r\n",
        "    widths = [512, 64, 64, 64, 64, 64]\r\n",
        "    strides = [(4, 1), (1, 1), (1, 1), (1, 1), (1, 1), (1, 1)]\r\n",
        "\r\n",
        "    x = Input(shape=(1024,), name='input', dtype='float32')\r\n",
        "    y = Reshape(target_shape=(1024, 1, 1), name='input-reshape')(x)\r\n",
        "\r\n",
        "    for l, f, w, s in zip(layers, filters, widths, strides):\r\n",
        "        y = Conv2D(f, (w, 1), strides=s, padding='same',\r\n",
        "                    activation='relu', name=\"conv%d\" % l)(y)\r\n",
        "        y = BatchNormalization(name=\"conv%d-BN\" % l)(y)\r\n",
        "        y = MaxPool2D(pool_size=(2, 1), strides=None, padding='valid',\r\n",
        "                        name=\"conv%d-maxpool\" % l)(y)\r\n",
        "        y = Dropout(0.25, name=\"conv%d-dropout\" % l)(y)\r\n",
        "\r\n",
        "    y = Permute((2, 1, 3), name=\"transpose\")(y)\r\n",
        "    y = Flatten(name=\"flatten\")(y)\r\n",
        "    y = Dense(360, activation='sigmoid', name=\"classifier\")(y)\r\n",
        "\r\n",
        "    model = Model(inputs=x, outputs=y)\r\n",
        "\r\n",
        "    if weights != None:\r\n",
        "        model.load_weights(weights)\r\n",
        "    model.compile('adam', 'binary_crossentropy', metrics=metrics)\r\n",
        "\r\n",
        "    models[model_capacity] = model\r\n",
        "\r\n",
        "    return model"
      ],
      "outputs": [],
      "metadata": {
        "id": "pamdqnF5M7Qe"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "source": [
        "def db_to_pow(db):\r\n",
        "  return 10**(db / 10)"
      ],
      "outputs": [],
      "metadata": {
        "id": "VDmcMvX3nzD8"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "source": [
        "def frame_energy(frame):\r\n",
        "  '''Calculates the average energy for a frame\r\n",
        "    \r\n",
        "    Parameters\r\n",
        "    ----------\r\n",
        "    frame : np.array\r\n",
        "      audio frame in np.float32 format\r\n",
        "\r\n",
        "    Returns\r\n",
        "    -------\r\n",
        "    average_energy : float\r\n",
        "      Average energy level for frame\r\n",
        "  '''\r\n",
        "\r\n",
        "  # Square the sample values to convert to energy values\r\n",
        "  energy = frame**2\r\n",
        "\r\n",
        "  # Sum the energy values to get total energy\r\n",
        "  total_energy = np.sum(energy)\r\n",
        "\r\n",
        "  # Divide by length to get average energy\r\n",
        "  return total_energy / len(frame)"
      ],
      "outputs": [],
      "metadata": {
        "id": "SIx7SuJTnzD8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Loading .wav files\n",
        "\n",
        "An issue we might need to discuss is the truncation of the data. Since not all of the steelpan audio is the same length, I've made it so that all the audio is just trimmed to the length of the shortest audio file.\n",
        "\n",
        "To generate the labels, each filename is parsed to get the MIDI pitch. Audio data is returned in a 2D array and labels are returned in a 1D array."
      ],
      "metadata": {
        "id": "vzqsVMpAPmM2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data formatting\n",
        "\n",
        "Audio is formatted according to how CREPE does it, but is formatted into batch format for input into model.fit(). Step size defaults to 10ms like in CREPE.\n",
        "\n",
        "The label data is scaled and encoded into one-hots to fit the model's bucketed output. There are 360 outputs, but only 29 actually appear as labels (MIDI pitches 60-89).\n",
        "\n",
        "Data is being trimmed by dBFS levels, which I've set to -30 for now. Anything -40 and below trims almost nothing, and -30 trims a good amount (I think it does need more tuning).\n",
        "\n",
        "The one-hotted labels have Gaussian blurring applied to them too (as specified in the other notebook)."
      ],
      "metadata": {
        "id": "LUIdeIZOQoTJ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "source": [
        "import soundfile, os\r\n",
        "def process_audio_file(filepath, threshold_pow, step_size=10):\r\n",
        "    '''\r\n",
        "        Processes a single audio file by formatting it into frames and extracting the label.\r\n",
        "\r\n",
        "        Parameters\r\n",
        "        ----------\r\n",
        "        filepath : string\r\n",
        "        Filepath for audio.\r\n",
        "\r\n",
        "        threshold_pow : float\r\n",
        "        Frames with average energy less than this are clipped.\r\n",
        "\r\n",
        "        step_size : int\r\n",
        "        Space between each frame (in milliseconds).\r\n",
        "\r\n",
        "        Returns\r\n",
        "        -------\r\n",
        "        frames : np.array dtype=float\r\n",
        "        The normalized and clipped audio, in frame form.\r\n",
        "\r\n",
        "        labels : int[]\r\n",
        "        The pitch of the audio file, in MIDI. It's duplicated\r\n",
        "        because the model requires one label for each frame.\r\n",
        "    '''\r\n",
        "    audio, _ = soundfile.read(filepath)\r\n",
        "\r\n",
        "    # Format the audio data into frames\r\n",
        "    hop_length = int(model_srate * step_size / 1000)\r\n",
        "    n_frames = 1 + int((len(audio) - 1024) / hop_length)\r\n",
        "    frames = as_strided(audio, shape=(1024, n_frames),\r\n",
        "                        strides=(audio.itemsize, hop_length * audio.itemsize))\r\n",
        "    frames = frames.transpose().copy()\r\n",
        "\r\n",
        "    # Find the first and last frames where levels meet threshold_pow\r\n",
        "    for f in range(len(frames)):\r\n",
        "        if frame_energy(frames[f]) > threshold_pow:\r\n",
        "            start_frame = f\r\n",
        "            break\r\n",
        "\r\n",
        "    for f in range(len(frames) - 1, -1, -1):\r\n",
        "        if frame_energy(frames[f]) > threshold_pow:\r\n",
        "            end_frame = f\r\n",
        "            break\r\n",
        "\r\n",
        "    frames = frames[start_frame:end_frame]\r\n",
        "\r\n",
        "    # Normalize each frame\r\n",
        "    frames -= np.mean(frames, axis=1)[:, np.newaxis]\r\n",
        "    frames /= np.std(frames, axis=1)[:, np.newaxis]\r\n",
        "\r\n",
        "    # Duplicate the label for these frames the set amount of times for the target data\r\n",
        "    labels = [int(os.path.basename(filepath).split(\"_\")[0]) for _ in range(end_frame - start_frame)]\r\n",
        "\r\n",
        "    return frames, labels\r\n",
        "    "
      ],
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "source": [
        "from os import walk\r\n",
        "import soundfile\r\n",
        "from numpy.lib.stride_tricks import as_strided\r\n",
        "\r\n",
        "def load_audio_batch(dir, threshold_db):\r\n",
        "    '''\r\n",
        "        Loads and processes all audio files from a directory, returning a single\r\n",
        "        array of audio frames and labels. Will load audio from nested dirs as well.\r\n",
        "\r\n",
        "        Parameters\r\n",
        "        ----------\r\n",
        "        dir : string\r\n",
        "        Starting dirpath to load audio from.\r\n",
        "\r\n",
        "        threshold_db : float\r\n",
        "        Frames with average energy less than this are clipped (this is converted dB -> power first).\r\n",
        "\r\n",
        "        Returns\r\n",
        "        -------\r\n",
        "        frames_list : np.array dtype=float\r\n",
        "        Audio frames from all loaded audio files.\r\n",
        "\r\n",
        "        labels_list : np.array dtype=float\r\n",
        "        The labels for the model. It is formatted as a one-hot vector\r\n",
        "        with Gaussian blur applied, so indices closer to the correct\r\n",
        "        index are closer to 1 (e.g. if the correct index is 200 then\r\n",
        "        index 198 might be 0.953 but index 50 might be 0.067).\r\n",
        "    '''\r\n",
        "\r\n",
        "    frames_list = []\r\n",
        "    labels_list = []\r\n",
        "    threshold_pow = db_to_pow(threshold_db)\r\n",
        "\r\n",
        "    for (dirpath, _, filenames) in walk(dir):\r\n",
        "        for filename in filenames:\r\n",
        "            frames, labels = process_audio_file(dirpath + \"/\" + filename, threshold_pow)\r\n",
        "\r\n",
        "            frames_list.append(frames)\r\n",
        "            labels_list += labels\r\n",
        "\r\n",
        "    # Assemble frames from all files into one array\r\n",
        "    frames_list = np.concatenate(frames_list)\r\n",
        "\r\n",
        "    # Convert MIDI to CREPE cents one-hot vector (0-360)\r\n",
        "    labels_list = np.array(tf.one_hot(5 * (np.array(labels_list) - 24), 360))\r\n",
        "\r\n",
        "    # Apply Gaussian blur\r\n",
        "    cents_i = np.arange(360)\r\n",
        "    for i in range(len(labels_list)):\r\n",
        "        cents_true = np.where(labels_list[i] == 1)[0][0]\r\n",
        "        labels_list[i] = np.exp(-((20 *(cents_i - cents_true)) ** 2) / (2 * (25 ** 2)))\r\n",
        "\r\n",
        "    return frames_list, labels_list"
      ],
      "outputs": [],
      "metadata": {
        "id": "UMdS1JKxM7Qf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model + Metrics\n",
        "\n",
        "I only know these three metrics, but there's probably some useful ones that I'm missing."
      ],
      "metadata": {
        "id": "M6wrQ6kqR7L6"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "source": [
        "metrics = [\"Accuracy\", \"Precision\", \"Recall\"]\r\n",
        "model = make_model(model_size, metrics, weights=\"model-\" + model_size + \".h5\")\r\n",
        "model.summary()"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"functional_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input (InputLayer)           [(None, 1024)]            0         \n",
            "_________________________________________________________________\n",
            "input-reshape (Reshape)      (None, 1024, 1, 1)        0         \n",
            "_________________________________________________________________\n",
            "conv1 (Conv2D)               (None, 256, 1, 512)       262656    \n",
            "_________________________________________________________________\n",
            "conv1-BN (BatchNormalization (None, 256, 1, 512)       2048      \n",
            "_________________________________________________________________\n",
            "conv1-maxpool (MaxPooling2D) (None, 128, 1, 512)       0         \n",
            "_________________________________________________________________\n",
            "conv1-dropout (Dropout)      (None, 128, 1, 512)       0         \n",
            "_________________________________________________________________\n",
            "conv2 (Conv2D)               (None, 128, 1, 64)        2097216   \n",
            "_________________________________________________________________\n",
            "conv2-BN (BatchNormalization (None, 128, 1, 64)        256       \n",
            "_________________________________________________________________\n",
            "conv2-maxpool (MaxPooling2D) (None, 64, 1, 64)         0         \n",
            "_________________________________________________________________\n",
            "conv2-dropout (Dropout)      (None, 64, 1, 64)         0         \n",
            "_________________________________________________________________\n",
            "conv3 (Conv2D)               (None, 64, 1, 64)         262208    \n",
            "_________________________________________________________________\n",
            "conv3-BN (BatchNormalization (None, 64, 1, 64)         256       \n",
            "_________________________________________________________________\n",
            "conv3-maxpool (MaxPooling2D) (None, 32, 1, 64)         0         \n",
            "_________________________________________________________________\n",
            "conv3-dropout (Dropout)      (None, 32, 1, 64)         0         \n",
            "_________________________________________________________________\n",
            "conv4 (Conv2D)               (None, 32, 1, 64)         262208    \n",
            "_________________________________________________________________\n",
            "conv4-BN (BatchNormalization (None, 32, 1, 64)         256       \n",
            "_________________________________________________________________\n",
            "conv4-maxpool (MaxPooling2D) (None, 16, 1, 64)         0         \n",
            "_________________________________________________________________\n",
            "conv4-dropout (Dropout)      (None, 16, 1, 64)         0         \n",
            "_________________________________________________________________\n",
            "conv5 (Conv2D)               (None, 16, 1, 128)        524416    \n",
            "_________________________________________________________________\n",
            "conv5-BN (BatchNormalization (None, 16, 1, 128)        512       \n",
            "_________________________________________________________________\n",
            "conv5-maxpool (MaxPooling2D) (None, 8, 1, 128)         0         \n",
            "_________________________________________________________________\n",
            "conv5-dropout (Dropout)      (None, 8, 1, 128)         0         \n",
            "_________________________________________________________________\n",
            "conv6 (Conv2D)               (None, 8, 1, 256)         2097408   \n",
            "_________________________________________________________________\n",
            "conv6-BN (BatchNormalization (None, 8, 1, 256)         1024      \n",
            "_________________________________________________________________\n",
            "conv6-maxpool (MaxPooling2D) (None, 4, 1, 256)         0         \n",
            "_________________________________________________________________\n",
            "conv6-dropout (Dropout)      (None, 4, 1, 256)         0         \n",
            "_________________________________________________________________\n",
            "transpose (Permute)          (None, 1, 4, 256)         0         \n",
            "_________________________________________________________________\n",
            "flatten (Flatten)            (None, 1024)              0         \n",
            "_________________________________________________________________\n",
            "classifier (Dense)           (None, 360)               369000    \n",
            "=================================================================\n",
            "Total params: 5,879,464\n",
            "Trainable params: 5,877,288\n",
            "Non-trainable params: 2,176\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "metadata": {
        "id": "r2zhlSGiM7Qg",
        "outputId": "13de043c-c2c7-49f3-8e02-fc9ca571b0ea"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Loading/Formatting data"
      ],
      "metadata": {
        "id": "uP4v_bqtSMWe"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "source": [
        "train, labels = load_audio_batch(\"tiny_16kHz/train\", -60)\r\n",
        "val, val_labels = load_audio_batch(\"tiny_16kHz/validation\", -60)"
      ],
      "outputs": [],
      "metadata": {
        "id": "8M3ZoJIiM7Qh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training\n",
        "\n",
        "Any batch size larger than 256 is slower, and also has trouble fitting into VRAM. 20 epochs was an arbitrary choice, which took roughly an hour and a half on a 1650 super.\n",
        "\n",
        "This also saves the model's weights after training."
      ],
      "metadata": {
        "id": "bbCYJaC4Sio2"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "source": [
        "history = model.fit(x=train, y=labels, batch_size=256, epochs=20, validation_data=(val, val_labels))\r\n",
        "model.save_weights('new-crepe-' + model_size + '.h5')"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "  2/300 [..............................] - ETA: 25s - loss: 0.0507 - accuracy: 0.0000e+00 - precision: 0.7580 - recall: 0.0465WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0490s vs `on_train_batch_end` time: 0.1221s). Check your callbacks.\n",
            "300/300 [==============================] - 54s 179ms/step - loss: 0.0141 - accuracy: 0.0000e+00 - precision: 0.9603 - recall: 0.0697 - val_loss: 0.0099 - val_accuracy: 0.0000e+00 - val_precision: 0.9814 - val_recall: 0.0863\n",
            "Epoch 2/20\n",
            "300/300 [==============================] - 51s 169ms/step - loss: 0.0090 - accuracy: 0.0000e+00 - precision: 0.9901 - recall: 0.0874 - val_loss: 0.0098 - val_accuracy: 0.0000e+00 - val_precision: 0.9897 - val_recall: 0.0877\n",
            "Epoch 3/20\n",
            "300/300 [==============================] - 52s 172ms/step - loss: 0.0086 - accuracy: 0.0000e+00 - precision: 0.9955 - recall: 0.0889 - val_loss: 0.0099 - val_accuracy: 0.0000e+00 - val_precision: 0.9913 - val_recall: 0.0879\n",
            "Epoch 4/20\n",
            "300/300 [==============================] - 51s 170ms/step - loss: 0.0084 - accuracy: 0.0000e+00 - precision: 0.9976 - recall: 0.0896 - val_loss: 0.0098 - val_accuracy: 0.0000e+00 - val_precision: 0.9930 - val_recall: 0.0881\n",
            "Epoch 5/20\n",
            "300/300 [==============================] - 51s 169ms/step - loss: 0.0083 - accuracy: 0.0000e+00 - precision: 0.9981 - recall: 0.0899 - val_loss: 0.0098 - val_accuracy: 0.0000e+00 - val_precision: 0.9925 - val_recall: 0.0884\n",
            "Epoch 6/20\n",
            "300/300 [==============================] - 52s 173ms/step - loss: 0.0082 - accuracy: 0.0000e+00 - precision: 0.9987 - recall: 0.0902 - val_loss: 0.0098 - val_accuracy: 0.0000e+00 - val_precision: 0.9923 - val_recall: 0.0885\n",
            "Epoch 7/20\n",
            "300/300 [==============================] - 51s 169ms/step - loss: 0.0082 - accuracy: 0.0000e+00 - precision: 0.9991 - recall: 0.0903 - val_loss: 0.0098 - val_accuracy: 0.0000e+00 - val_precision: 0.9921 - val_recall: 0.0887\n",
            "Epoch 8/20\n",
            "300/300 [==============================] - 51s 169ms/step - loss: 0.0081 - accuracy: 0.0000e+00 - precision: 0.9991 - recall: 0.0905 - val_loss: 0.0097 - val_accuracy: 0.0000e+00 - val_precision: 0.9938 - val_recall: 0.0888\n",
            "Epoch 9/20\n",
            "300/300 [==============================] - 51s 171ms/step - loss: 0.0081 - accuracy: 0.0000e+00 - precision: 0.9993 - recall: 0.0906 - val_loss: 0.0098 - val_accuracy: 0.0000e+00 - val_precision: 0.9928 - val_recall: 0.0886\n",
            "Epoch 10/20\n",
            "300/300 [==============================] - 50s 167ms/step - loss: 0.0081 - accuracy: 0.0000e+00 - precision: 0.9994 - recall: 0.0906 - val_loss: 0.0097 - val_accuracy: 0.0000e+00 - val_precision: 0.9947 - val_recall: 0.0890\n",
            "Epoch 11/20\n",
            "300/300 [==============================] - 51s 170ms/step - loss: 0.0081 - accuracy: 0.0000e+00 - precision: 0.9993 - recall: 0.0906 - val_loss: 0.0098 - val_accuracy: 0.0000e+00 - val_precision: 0.9939 - val_recall: 0.0889\n",
            "Epoch 12/20\n",
            "300/300 [==============================] - 51s 170ms/step - loss: 0.0081 - accuracy: 0.0000e+00 - precision: 0.9994 - recall: 0.0907 - val_loss: 0.0099 - val_accuracy: 0.0000e+00 - val_precision: 0.9934 - val_recall: 0.0888\n",
            "Epoch 13/20\n",
            "300/300 [==============================] - 53s 176ms/step - loss: 0.0081 - accuracy: 0.0000e+00 - precision: 0.9995 - recall: 0.0907 - val_loss: 0.0097 - val_accuracy: 0.0000e+00 - val_precision: 0.9947 - val_recall: 0.0889\n",
            "Epoch 14/20\n",
            "300/300 [==============================] - 52s 172ms/step - loss: 0.0081 - accuracy: 0.0000e+00 - precision: 0.9997 - recall: 0.0907 - val_loss: 0.0097 - val_accuracy: 0.0000e+00 - val_precision: 0.9962 - val_recall: 0.0891\n",
            "Epoch 15/20\n",
            "300/300 [==============================] - 51s 170ms/step - loss: 0.0081 - accuracy: 0.0000e+00 - precision: 0.9995 - recall: 0.0907 - val_loss: 0.0097 - val_accuracy: 0.0000e+00 - val_precision: 0.9952 - val_recall: 0.0890\n",
            "Epoch 16/20\n",
            "300/300 [==============================] - 50s 168ms/step - loss: 0.0080 - accuracy: 0.0000e+00 - precision: 0.9997 - recall: 0.0908 - val_loss: 0.0097 - val_accuracy: 0.0000e+00 - val_precision: 0.9944 - val_recall: 0.0890\n",
            "Epoch 17/20\n",
            "300/300 [==============================] - 51s 169ms/step - loss: 0.0080 - accuracy: 0.0000e+00 - precision: 0.9997 - recall: 0.0908 - val_loss: 0.0097 - val_accuracy: 0.0000e+00 - val_precision: 0.9946 - val_recall: 0.0892\n",
            "Epoch 18/20\n",
            "300/300 [==============================] - 52s 174ms/step - loss: 0.0080 - accuracy: 0.0000e+00 - precision: 0.9996 - recall: 0.0908 - val_loss: 0.0097 - val_accuracy: 0.0000e+00 - val_precision: 0.9955 - val_recall: 0.0895\n",
            "Epoch 19/20\n",
            "300/300 [==============================] - 50s 166ms/step - loss: 0.0080 - accuracy: 0.0000e+00 - precision: 0.9995 - recall: 0.0908 - val_loss: 0.0097 - val_accuracy: 0.0000e+00 - val_precision: 0.9954 - val_recall: 0.0892\n",
            "Epoch 20/20\n",
            "300/300 [==============================] - 50s 166ms/step - loss: 0.0080 - accuracy: 0.0000e+00 - precision: 0.9999 - recall: 0.0908 - val_loss: 0.0097 - val_accuracy: 0.0000e+00 - val_precision: 0.9960 - val_recall: 0.0894\n"
          ]
        }
      ],
      "metadata": {
        "id": "jyDFC64UM7Qi",
        "outputId": "f50e29f5-1abf-4c3e-b058-1cf47803b544"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Results\n",
        "\n",
        "This model has pretty normal graphs except for accuracy, which confuses me so much. Please take a look and see if you can find anything wrong with the data formatting or training, as the accuraccy *does* improve over epochs, but starts out at near 0. Precision and Recall are also suspiciously high, at 0.99+ each. \n",
        "\n",
        "It's possible this is simply because I used the wrong metrics, as the model seems to be pretty accurate for individual files from the training/validation set (see below)."
      ],
      "metadata": {
        "id": "T35K66bzTArf"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "source": [
        "from matplotlib import pyplot as plt\r\n",
        "def plot(data, labels, x, y):\r\n",
        "    '''Plot statistics. Takes a list of lists and list of labels.'''\r\n",
        "    \r\n",
        "    print(y + \" vs. \" + x)\r\n",
        "    for i in range(len(data)):\r\n",
        "        plt.plot(data[i], label=labels[i])\r\n",
        "    plt.xlabel(x)\r\n",
        "    plt.ylabel(y)\r\n",
        "    plt.legend()\r\n",
        "    plt.show()\r\n",
        "    print()"
      ],
      "outputs": [],
      "metadata": {
        "id": "DsTOkacZM7Qi"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "source": [
        "plot((history.history[\"loss\"], ), (\"Loss\", ), \"Epoch\", \"Loss\")\r\n",
        "plot((history.history[\"accuracy\"], history.history[\"precision\"], history.history[\"recall\"], ), (\"Accuracy\", \"Precision\", \"Recall\", ), \"Epoch\", \"Accuracy, Precision, Recall\")"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss vs. Epoch\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Accuracy, Precision, Recall vs. Epoch\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEGCAYAAABo25JHAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8QVMy6AAAACXBIWXMAAAsTAAALEwEAmpwYAAAkxklEQVR4nO3deZRU9Zn/8fdDN9igLLKMo4KhdSDI0m2gIa4D7oAoEmLcIsZR+RGXxMkxP5kzExQzJ2c0yWhARoKRxBgDToy45IgxgE7yCyq0DqtiBERpcWGRBmTt7uf3x73dFtVV1beXW9V0fV6cOnXv/X7vvc+9XdRTd/t+zd0REZH81S7XAYiISG4pEYiI5DklAhGRPKdEICKS55QIRETyXGGuA2isnj17et++fXMdhojIEeWNN97Y5u69UpUdcYmgb9++lJeX5zoMEZEjipm9n65Mp4ZERPKcEoGISJ5TIhARyXNKBCIieU6JQEQkz8WWCMxsrpl9amZr0pSbmc0ws/VmtsrMhsYVi4iIpBfnEcGvgNEZyscA/cLXZODhGGMREZE0YnuOwN3/bGZ9M1QZD/zag3awXzOzbmZ2vLt/FFdMRyR3qKmC6oPByx285ot3PGk4VZknlKV7JZenqp+03JTvNWnK+OL9i42rv62Ryj39cF29dMMZd3YD5alY0qhlLk+3rsNi8zTT09XPNE+q6U3ZZ8nrs3Bbw+2rHa7b/jTlyetItd6001pAvb9PvQqNmN9STE81LXF6pv87te8Z6n3pDDjlvAa2ofFy+UDZicDmhPGKcFq9RGBmkwmOGjjppJOyElyjuMOBXbBvJ+zfmf59/y6o2v/Fl3rVQag+kDR8CKoOJNQ5QNO+oESkzTn7n9tcIkiVelN+47n7HGAOQFlZWfa/FasPwYYl8O6fYO+2FF/wleEv5TSsADp2g6KuUFgEBR2CV+FR0L5b8F7QHgqOgsIOwXtBhy+GCzt8MY8VfPFLw9ol/BILh60ddb/G6g3XzpPpZQ2XYeFfL+FXX/L6Mr0ftm8a8wuML+ZP+WszsX6qX6cJ80deXwZNOsJJ94uRDGURpqdcXoZ5mrLPasfr/Upv6Fd80i/6VEcNUac1SwNfHY05Ykx11BXlSCzK/49U7435XDZBLhNBBdAnYbw3sCVHsdTnDh+vhpXzYfV/w+dboUNn6HwcFHWDTj2g+ynhF3y3zO8djon9Dyki0lS5TATPAbeZ2Xzgq0Blq7g+sPtjWPXfQQL4dC20aw9fHg2l18A/XBD8OhcRaUNiSwRmNg8YBfQ0swrgbqA9gLvPBl4AxgLrgb3ADXHF0qCDe+GdF2DlvOAUkNdA7+FwyU9h0NegU/echSYiErc47xq6uoFyB26Na/0NqqmBD5YGX/5rn4WDu6FrHzj7e1B6FfTsl7PQRESy6YhrhrrZtm8ITvusmg87PwjO3w+8PPjy/9JZ0E4PW4tIfsmfRPD+UvjT3VCxLLi75eRRcN4PYMAl0OHoXEcnIpIz+ZMI2hUG9/pfMB1KvgFdTsh1RCIirUL+JILew+GW13Qbp4hIkvxJBEoAIiIp6cqoiEieUyIQEclzSgQiInlOiUBEJM8pEYiI5DklAhGRPKdEICKS55QIRETynBKBiEieUyIQEclzSgQiInlOiUBEJM8pEYiI5DklAhGRPKdEICKS55QIRETynBKBiEieUyIQEclzSgQiInlOiUBEJM8pEYiI5DklAhGRPKdEICKS55QIRETyXGG6AjPbDXiqIsDdvUtsUYmISNakTQTu3jmbgYiISG6kPTVkZt0zvaIs3MxGm9k7ZrbezKamKO9qZs+b2UozW2tmNzRnY0REpPHSHhEAbxCcGrIUZQ6cnGnBZlYAzAIuBCqA5Wb2nLu/lVDtVuAtd7/UzHoB75jZE+5+sDEbISIiTZfp1FBxM5c9Aljv7hsBzGw+MB5ITAQOdDYzA44BdgBVzVyviIg0QqYjgjpmdizQDyiqnebuf25gthOBzQnjFcBXk+o8BDwHbAE6A1e6e02K9U8GJgOcdNJJUUIWEZGIGrx91MxuAv4M/BGYHr7fE2HZ6U4pJboYWAGcAJwGPGRm9e5Gcvc57l7m7mW9evWKsGoREYkqynME3wWGA++7+7nAV4CtEearAPokjPcm+OWf6AbgaQ+sB94DBkRYtoiItJAoiWC/u+8HMLOj3H0d8OUI8y0H+plZsZl1AK4iOA2U6APg/HDZx4XL3Rg1eBERab4o1wgqzKwb8AzwJzP7jPq/7Otx9yozu43gVFIBMNfd15rZlLB8NvBD4FdmtprgVNJd7r6tSVsiIiJNYu6pHh5OU9lsJNAVeDFXt3iWlZV5eXl5LlYtInLEMrM33L0sVVmUi8Wnm1lnAHf/H+BlgusEIiLSBkS5RvAwsCdh/PNwmoiItAFREoF5wvmj8D7/SM8fiIhI6xclEWw0s++YWfvw9V10Z4+ISJsRJRFMAc4EPuSLp4MnxxmUiIhkT4OneNz9U4JnAEREpA2KctdQfzNbbGZrwvESM/u3+EMTEZFsiHJq6BHgX4BDAO6+Ch0hiIi0GVESQSd3X5Y0TU1Fi4i0EVESwTYzO4Ww5VAz+zrwUaxRiYhI1kR5HuBWYA4wwMw+JGgh9NpYoxIRkayJctfQRuACMzua4AhiH3Al8H7MsYmISBZk6ry+i5n9i5k9ZGYXAnuB64H1wDeyFaCIiMQr0xHB48BnwKvAzcD/BToAl7v7ivhDExGRbMiUCE529yEAZvYLYBtwkrvvzkpkIiKSFZnuGjpUO+Du1cB7SgIiIm1PpiOCUjPbFQ4b0DEcN8DdvV4n8yIicuRJmwjcvSCbgYiISG5EeaBMRETaMCUCEZE8p0QgIpLnlAhERPJcoxOBmS0ys4VmNi6OgEREJLua0gn9JOB44PQWjkVERHKg0YnA3bcAW4A3Wj4cERHJtgYTgZmdBdwDfCmsX/tA2cnxhiYiItkQ5YjgUeCfCY4AquMNR0REsi1KIqh094WxRyIiIjkRJRG8bGY/Bp4GDtROdPc3Y4tKRESyJkoi+Gr4XpYwzYHzWj4cEclnhw4doqKigv379+c6lCNWUVERvXv3pn379pHnidJV5blNDcjMRgM/AwqAX7j7f6SoMwp4EGgPbHP3kU1dn4gc2SoqKujcuTN9+/bFzHIdzhHH3dm+fTsVFRUUFxdHnq/BB8rMrKuZ/aeZlYevn5pZ1wjzFQCzgDHAQOBqMxuYVKcb8F/AZe4+CLgicuQi0ubs37+fHj16KAk0kZnRo0ePRh9RRXmyeC6wm6Cf4m8Au4BfRphvBLDe3Te6+0FgPjA+qc41wNPu/gGAu38aNXARaZuUBJqnKfsvSiI4xd3vDr/QN7r7dCDKMwQnApsTxivCaYn6A8ea2Stm9oaZTUq1IDObXHtEsnXr1girFhGRqKIkgn1mdnbtSPiA2b4I86VKS540XggMAy4BLgZ+YGb9683kPsfdy9y9rFevXhFWLSLSdAsWLMDMWLduXa5DyYooieDbwCwz22Rm7wMPAVMizFcB9EkY703QNEVynRfd/XN33wb8GSiNsGwRkdjMmzePs88+m/nz58e2jurq1vN8boOJwN1XuHspUAIMcfevuPvKCMteDvQzs2Iz6wBcBTyXVOdZ4BwzKzSzTgS3qr7duE0QEWk5e/bs4a9//SuPPvpoXSKorq7mzjvvZMiQIZSUlDBz5kwAli9fzplnnklpaSkjRoxg9+7d/OpXv+K2226rW964ceN45ZVXADjmmGOYNm0aX/3qV3n11Ve59957GT58OIMHD2by5Mm4BydN1q9fzwUXXEBpaSlDhw5lw4YNXHfddTz77LN1y7322mt57rnkr9SmSXv7qJl9091/Y2bfS5oOgLv/Z6YFu3uVmd0G/JHg9tG57r7WzKaE5bPd/W0zexFYBdQQ3GK6pllbJCJtwvTn1/LWll0tusyBJ3Th7ksHZazzzDPPMHr0aPr370/37t158803ef3113nvvff43//9XwoLC9mxYwcHDx7kyiuv5Mknn2T48OHs2rWLjh07Zlz2559/zuDBg7n33nuDeAYOZNq0aQBcd911/OEPf+DSSy/l2muvZerUqUyYMIH9+/dTU1PDTTfdxAMPPMD48eOprKxk6dKlPPbYYy2yXzI9R3B0+N65qQt39xeAF5KmzU4a/zHw46auQ0SkJc2bN4877rgDgKuuuop58+axceNGpkyZQmFh8JXZvXt3Vq9ezfHHH8/w4cMB6NKlS4PLLigoYOLEiXXjL7/8Mvfffz979+5lx44dDBo0iFGjRvHhhx8yYcIEIHhADGDkyJHceuutfPrppzz99NNMnDixLp7mSrsUd/95+D69RdYkItIIDf1yj8P27dtZsmQJa9aswcyorq7GzBg2bFi92zLdPeWtmoWFhdTU1NSNJ97TX1RUREFBQd30W265hfLycvr06cM999zD/v37604PpXLdddfxxBNPMH/+fObOndvcza0T5YGy+82si5m1N7PFZrbNzL7ZYhGIiLQSTz31FJMmTeL9999n06ZNbN68meLiYoYOHcrs2bOpqqoCYMeOHQwYMIAtW7awfPlyAHbv3k1VVRV9+/ZlxYoV1NTUsHnzZpYtW5ZyXbUJomfPnuzZs4ennnoKCI4sevfuzTPPPAPAgQMH2Lt3LwDf+ta3ePDBBwEYNKjlEmWUu4YucvddwDiCu3z6A99vsQhERFqJefPm1Z2SqTVx4kS2bNnCSSedRElJCaWlpfz2t7+lQ4cOPPnkk9x+++2UlpZy4YUXsn//fs466yyKi4sZMmQId955J0OHDk25rm7dunHzzTczZMgQLr/88rpTTACPP/44M2bMoKSkhDPPPJOPP/4YgOOOO45TTz2VG264oUW32zIdhgCY2Vp3H2RmjwC/d/cXzWxleCdR1pWVlXl5eXkuVi0iMXv77bc59dRTcx1Gq7V3716GDBnCm2++Sdeu6Vv6SbUfzewNdy9LVT/KEcHzZraOoPXRxWbWC1DTgCIiWbRo0SIGDBjA7bffnjEJNEWU1kenmtl9wC53rzazz6nfZpCIiMToggsu4IMPPohl2ZmeIzjP3ZeY2dcSpiVWeTqWiEREJKsyHRGMBJYAl6Yoc5QIRETahEzPEdwdvrfs5WkREWlVojxH8KOwA5na8WPN7N9jjUpERLImyl1DY9x9Z+2Iu38GjI0tIhGRHCooKOC0005j8ODBXHHFFXUPczXHtGnTWLRoUdry2bNn8+tf/7rZ62mqKA1VFJjZUe5+AMDMOgJHxRuWiEhudOzYkRUrVgBBC5+zZ8/me9/7ou3N6urqumYioqptZC6dKVOitOwfnyhHBL8heH7gRjP7J+BPQMs0eSci0oqdc845rF+/nldeeYVzzz2Xa665hiFDhlBdXc33v/99hg8fTklJCT//+c/r5rn//vsZMmQIpaWlTJ06FQiahqhtQmLq1KkMHDiQkpIS7rzzTgDuuecefvKTnwCwYsUKTj/9dEpKSpgwYQKfffYZAKNGjeKuu+5ixIgR9O/fn7/85S8ttp1RniO438xWARcQ9Dr2Q3f/Y4tFICKSysKp8PHqll3m3w+BMf8RqWpVVRULFy5k9OjRACxbtow1a9ZQXFzMnDlz6Nq1K8uXL+fAgQOcddZZXHTRRaxbt45nnnmG119/nU6dOrFjx47Dlrljxw4WLFjAunXrMDN27txZb72TJk1i5syZjBw5kmnTpjF9+vS69oWqqqpYtmwZL7zwAtOnT894uqkxorZh+jZQ5e6LzKyTmXV2990tEoGISCuyb98+TjvtNCA4IrjxxhtZunQpI0aMoLi4GICXXnqJVatW1f3Kr6ys5N1332XRokXccMMNdOrUCQiaq07UpUsXioqKuOmmm7jkkksYN27cYeWVlZXs3LmTkSNHAnD99ddzxRVX1JV/7WvBY13Dhg1j06ZNLbbNDSYCM7sZmAx0B04h6IB+NnB+i0UhIpIs4i/3lpZ4jSDR0UcfXTfs7sycOZOLL774sDovvvhiyqapaxUWFrJs2TIWL17M/Pnzeeihh1iyZEnk2I46Krg8W1BQUNcSakuIco3gVuAsYBeAu78L/F2LRSAicoS5+OKLefjhhzl06BAAf/vb3/j888+56KKLmDt3bt2dRsmnhvbs2UNlZSVjx47lwQcfrJdwunbtyrHHHlt3/v/xxx+vOzqIU5RTQwfc/WBtljOzQoIni0VE8tJNN93Epk2bGDp0KO5Or1696rq4XLFiBWVlZXTo0IGxY8fyox/9qG6+3bt3M378+LoOaB544IF6y37ssceYMmUKe/fu5eSTT+aXv/xl7NsTpRnq+4GdwCTgduAW4C13/9fYo0tBzVCLtF1qhrplxNEM9V3AVmA18H8I+iD+t2bGKSIirUTGU0Nm1g5Y5e6DgUeyE5KIiGRTxiMCd68BVprZSVmKR0REsizKxeLjgbVmtgz4vHaiu18WW1QiIpI1URLB9NijEBGRnMnUQ1kRMAX4B4ILxY+6e8s9wSAiIq1CpmsEjxF0WL8aGAP8NCsRiYjkUGIz1JdeemnK9oCao2/fvmzbtg2AY445pkWX3VSZEsFAd/+mu/8c+DpwTpZiEhHJmdomJtasWUP37t2ZNWtWrkOKXaZEcKh2QKeERCQfnXHGGXz44YcAbNiwgdGjRzNs2DDOOecc1q1bB8Ann3zChAkTKC0tpbS0lKVLlwJw+eWXM2zYMAYNGsScOXNytg1RZLpYXGpmu8JhAzqG4wa4u3eJPToRyVv3LbuPdTvWtegyB3QfwF0j7opUt7q6msWLF3PjjTcCMHnyZGbPnk2/fv14/fXXueWWW1iyZAnf+c53GDlyJAsWLKC6upo9e/YAMHfuXLp3786+ffsYPnw4EydOpEePHi26PS0lU+f1jeuCR0SkDahthnrTpk0MGzaMCy+8kD179rB06dLDmoQ+cOAAAEuWLKnrZrKgoICuXbsCMGPGDBYsWADA5s2beffdd4+8RCAikktRf7m3tNprBJWVlYwbN45Zs2bxrW99i27duqVsnjqVV155hUWLFvHqq6/SqVMnRo0axf79++MNvBmitDXUZGY22szeMbP1ZjY1Q73hZlZtZl+PMx4Rkai6du3KjBkz+MlPfkLHjh0pLi7md7/7HRD0R7By5UoAzj//fB5++GEgOJ20a9cuKisrOfbYY+nUqRPr1q3jtddey9l2RBFbIjCzAmAWwa2nA4GrzWxgmnr3Aer+UkRala985SuUlpYyf/58nnjiCR599FFKS0sZNGgQzz77LAA/+9nPePnllxkyZAjDhg1j7dq1jB49mqqqKkpKSvjBD37A6aefnuMtySzOU0MjgPXuvhHAzOYD44G3kurdDvweGB5jLCIikdRe7K31/PPP1w2/+OKL9eofd9xxdUkh0cKFC1MuP7GLyeR15UqjjwjMbJGZLTSzcQ1UPRHYnDBeEU5LXNaJwASCri8zrXOymZWbWfnWrVsbG7KIiGTQlFNDkwj6I/hSA/VSddyZ3AvOg8Bd7l6daUHuPsfdy9y9rFevXpEDFRGRhkXpvH4c8ELYJDXuvgXYArzRwKwVQJ+E8d7hfInKgPlhN5g9gbFmVuXuz0SKXkTaHHfP2AG8ZNZQr5OpRDkiuAp418zuN7PG9CG3HOhnZsVm1iFcznOJFdy92N37untf4CngFiUBkfxVVFTE9u3bm/RlJkES2L59O0VFRY2ar8EjAnf/ppl1Aa4GfmlmDvwSmOfuuzPMV2VmtxHcDVQAzHX3tWY2JSzPeF1ARPJP7969qaioQNcCm66oqIjevXs3ap4GO6+vq2jWE/gmcAfwNkHz1DPcfWbjwmwedV4vItJ4zeq83swuNbMFwBKgPTDC3ccApcCdLRqpiIhkXZTnCK4AHnD3PydOdPe9ZvZP8YQlIiLZEiUR3A18VDtiZh2B49x9k7svji0yERHJiih3Df0OqEkYrw6niYhIGxAlERS6+8HakXC4Q3whiYhINkVJBFvN7LLaETMbD2yLLyQREcmmKNcIpgBPmNlDBM1GbCZoZkJERNqAKA+UbQBON7NjCJ47SPsQmYiIHHkiNUNtZpcAg4Ci2jZA3P3eGOMSEZEsifJA2WzgSoJ+A4zguYKGWh4VEZEjRJSLxWe6+yTgM3efDpzB4a2KiojIESxKIqjtcXmvmZ0AHAKK4wtJRESyKco1gufNrBvwY+BNgs5lHokzKBERyZ6MicDM2gGL3X0n8Hsz+wNQ5O6V2QhORETil/HUUNgr2U8Txg8oCYiItC1RrhG8ZGYTTX3HiYi0SVGuEXwPOBqoMrP9BLeQurt3iTUyERHJiihPFnfORiAiIpIbDSYCM/vHVNOTO6oREZEjU5RTQ99PGC4CRgBvAOfFEpGIiGRVlFNDlyaOm1kf4P7YIhIRkayKctdQsgpgcEsHIiIiuRHlGsFMgqeJIUgcpwErY4xJRESyKMo1gvKE4Spgnrv/NaZ4REQky6IkgqeA/e5eDWBmBWbWyd33xhuaiIhkQ5RrBIuBjgnjHYFF8YQjIiLZFiURFLn7ntqRcLhTfCGJiEg2RUkEn5vZ0NoRMxsG7IsvJBERyaYo1wjuAH5nZlvC8eMJuq4UEZE2IMoDZcvNbADwZYIG59a5+6HYIxMRkayI0nn9rcDR7r7G3VcDx5jZLVEWbmajzewdM1tvZlNTlF9rZqvC11IzK238JoiISHNEuUZwc9hDGQDu/hlwc0MzmVkBMAsYAwwErjazgUnV3gNGunsJ8ENgTsS4RUSkhURJBO0SO6UJv+A7RJhvBLDe3Te6+0FgPjA+sYK7Lw0TC8BrQO9oYYuISEuJkgj+CPy3mZ1vZucB84AXI8x3IrA5YbwinJbOjcDCVAVmNtnMys2sfOvWrRFWLSIiUUW5a+guYDLwbYKLxS8Bj0SYL1XXlp5iGmZ2LkEiODtVubvPITxtVFZWlnIZIiLSNA0eEbh7jbvPdvevu/tEYC0wM8KyK4A+CeO9gS3JlcysBPgFMN7dt0cLW0REWkqUIwLM7DTgaoLnB94Dno4w23Kgn5kVAx8CVwHXJC33pHBZ17n736KHLSIiLSVtIjCz/gRf3lcD24EnAXP3c6Ms2N2rzOw2gmsMBcBcd19rZlPC8tnANKAH8F/h9egqdy9rxvaIiEgjmXvqU+5mVgP8BbjR3deH0za6+8lZjK+esrIyLy8vb7iiiIjUMbM30v3QznSNYCLwMfCymT1iZueT+gKwiIgcwdImAndf4O5XAgOAV4B/Bo4zs4fN7KIsxSciIjGLctfQ5+7+hLuPI7jzZwVQr7kIERE5MjWq83p33+HuP3f38+IKSEREsqtRiUBERNoeJQIRkTynRCAikueUCERE8pwSgYhInlMiEBHJc0oEIiJ5TolARCTPKRGIiOQ5JQIRkTynRCAikuci9VAm4O5s3beVA9UHqKqporqmmiqvoqom4ZUwXl1TzSE/dNh4VU0VNdRQ41+83J1qr8ZxarwmGHY/rE4NCfXC/iM87P7Z3XG87j1xWm29xHnS1kusk1QP/2J6ct3kfZQc22HjCcurm4fM8zQ4n6eo18A8h8Wcuhtt0vXT0Rzp1tXUdabb/1HXl20NxtO6wq2nufszyt+3oXVcdsplXHPqNRnrNIUSQRq7D+5mzbY1rNq6ilXbVrFq6yp2HtiZtfUbRjtrh5lRYAXBMIaZUfuvtneI5Olhb29BHag3T3KdxPF6y0uxrMPGk6bXLaOBerXjUeZJO1/C9ifGnnGeFJJjT7WslpC8n1NUaDjW5PJ6o5ZxPNcybj+tL956mhlelO3LVKdjYcfmBZCGEgFQXVPNhsoNwZd++NpYubEuO5/S9RTO7XMup/Y4lU6FnShsV0hBuwLaW3sK2xXWjRdaMNy+XfvDxhNf7awd7Uj6gg+HzYx2tKubJiKSDXmZCLbv287qbavrvvRXb1vN3qq9AHQ7qhslvUoYUzyGIb2GMLjnYLp06JLjiEVE4pM3iWD11tX85u3fsGrrKir2VABQaIX0796fy065jJJeJZT2KqVP5z76NS4ieSVvEsHug7sp/7ic0r8r5covX0lJrxJO7XFqbOfcRESOFHmTCM444QwWf2NxrsMQEWl18uY5Ap3uERFJLW8SgYiIpKZEICKS55QIRETynBKBiEieUyIQEclzSgQiInlOiUBEJM8pEYiI5LlYE4GZjTazd8xsvZlNTVFuZjYjLF9lZkPjjEdEROqLLRGYWQEwCxgDDASuNrOBSdXGAP3C12Tg4bjiERGR1OJsa2gEsN7dNwKY2XxgPPBWQp3xwK896LrnNTPrZmbHu/tHLR3M9OfX8taWXS29WBGRrBl4QhfuvnRQiy83zlNDJwKbE8YrwmmNrYOZTTazcjMr37p1a4sHKiKSz+I8IkjVyltyh5xR6uDuc4A5AGVlZU3qODSOLCoi0hbEeURQAfRJGO8NbGlCHRERiVGciWA50M/Mis2sA3AV8FxSneeASeHdQ6cDlXFcHxARkfRiOzXk7lVmdhvwR6AAmOvua81sSlg+G3gBGAusB/YCN8QVj4iIpBZrD2Xu/gLBl33itNkJww7cGmcMIiKSmZ4sFhHJc0oEIiJ5TolARCTPKRGIiOQ5C67XHjnMbCvwfhNn7wlsa8FwWlprjw9af4yKr3kUX/O05vi+5O69UhUccYmgOcys3N3Lch1HOq09Pmj9MSq+5lF8zdPa40tHp4ZERPKcEoGISJ7Lt0QwJ9cBNKC1xwetP0bF1zyKr3lae3wp5dU1AhERqS/fjghERCSJEoGISJ5rk4nAzEab2Ttmtt7MpqYoNzObEZavMrOhWYytj5m9bGZvm9laM/tuijqjzKzSzFaEr2nZii9c/yYzWx2uuzxFeS7335cT9ssKM9tlZnck1cn6/jOzuWb2qZmtSZjW3cz+ZGbvhu/Hppk34+c1xvh+bGbrwr/hAjPrlmbejJ+HGOO7x8w+TPg7jk0zb67235MJsW0ysxVp5o19/zWbu7epF0GT1xuAk4EOwEpgYFKdscBCgh7STgdez2J8xwNDw+HOwN9SxDcK+EMO9+EmoGeG8pztvxR/648JHpTJ6f4D/hEYCqxJmHY/MDUcngrcl2YbMn5eY4zvIqAwHL4vVXxRPg8xxncPcGeEz0BO9l9S+U+Babnaf819tcUjghHAenff6O4HgfnA+KQ644Ffe+A1oJuZHZ+N4Nz9I3d/MxzeDbxNin6aW7mc7b8k5wMb3L2pT5q3GHf/M7AjafJ44LFw+DHg8hSzRvm8xhKfu7/k7lXh6GsEPQTmRJr9F0XO9l8tMzPgG8C8ll5vtrTFRHAisDlhvIL6X7RR6sTOzPoCXwFeT1F8hpmtNLOFZpbtDpcdeMnM3jCzySnKW8X+I+j1Lt1/vlzuv1rHedjjXvj+dynqtJZ9+U8ER3mpNPR5iNNt4amruWlOrbWG/XcO8Im7v5umPJf7L5K2mAgsxbTke2Sj1ImVmR0D/B64w913JRW/SXC6oxSYCTyTzdiAs9x9KDAGuNXM/jGpvDXsvw7AZcDvUhTnev81RmvYl/8KVAFPpKnS0OchLg8DpwCnAR8RnH5JlvP9B1xN5qOBXO2/yNpiIqgA+iSM9wa2NKFObMysPUESeMLdn04ud/dd7r4nHH4BaG9mPbMVn7tvCd8/BRYQHH4nyun+C40B3nT3T5ILcr3/EnxSe8osfP80RZ1cfxavB8YB13p4QjtZhM9DLNz9E3evdvca4JE06831/isEvgY8ma5OrvZfY7TFRLAc6GdmxeGvxquA55LqPAdMCu9+OR2orD2Ej1t4PvFR4G13/880df4+rIeZjSD4O23PUnxHm1nn2mGCC4prkqrlbP8lSPsrLJf7L8lzwPXh8PXAsynqRPm8xsLMRgN3AZe5+940daJ8HuKKL/G604Q0683Z/gtdAKxz94pUhbncf42S66vVcbwI7mr5G8HdBP8aTpsCTAmHDZgVlq8GyrIY29kEh66rgBXha2xSfLcBawnugHgNODOL8Z0crndlGEOr2n/h+jsRfLF3TZiW0/1HkJQ+Ag4R/Eq9EegBLAbeDd+7h3VPAF7I9HnNUnzrCc6v134OZyfHl+7zkKX4Hg8/X6sIvtyPb037L5z+q9rPXULdrO+/5r7UxISISJ5ri6eGRESkEZQIRETynBKBiEieUyIQEclzSgQiInlOiUAkiZlV2+EtnLZYi5Zm1jexBUuR1qAw1wGItEL73P20XAchki06IhCJKGxX/j4zWxa+/iGc/iUzWxw2jrbYzE4Kpx8XtvO/MnydGS6qwMwesaA/ipfMrGPONkoEJQKRVDomnRq6MqFsl7uPAB4CHgynPUTQLHcJQcNtM8LpM4D/8aDxu6EET5YC9ANmufsgYCcwMdatEWmAniwWSWJme9z9mBTTNwHnufvGsOHAj929h5ltI2j+4FA4/SN372lmW4He7n4gYRl9gT+5e79w/C6gvbv/exY2TSQlHRGINI6nGU5XJ5UDCcPV6Fqd5JgSgUjjXJnw/mo4vJSg1UuAa4H/Fw4vBr4NYGYFZtYlW0GKNIZ+iYjU1zGpI/IX3b32FtKjzOx1gh9RV4fTvgPMNbPvA1uBG8Lp3wXmmNmNBL/8v03QgqVIq6JrBCIRhdcIytx9W65jEWlJOjUkIpLndEQgIpLndEQgIpLnlAhERPKcEoGISJ5TIhARyXNKBCIiee7/A8XtF9e/MfioAAAAAElFTkSuQmCC",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n"
          ]
        }
      ],
      "metadata": {
        "id": "5eodNFe6M7Qj",
        "outputId": "74354b83-d530-4ff7-ca4e-3f7ab8692b20"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Testing for individual sound files\n",
        "\n",
        "The model seems to be pretty good for the first couple of seconds, but then becomes very inaccurate as the pitch fades."
      ],
      "metadata": {
        "id": "5kdaOzg3srSD"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "source": [
        "import soundfile\r\n",
        "def load_audio(file):\r\n",
        "    wav, sr = soundfile.read(file)\r\n",
        "    return wav, sr"
      ],
      "outputs": [],
      "metadata": {
        "id": "qdMWs4vsM7Qk"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "source": [
        "from numpy.lib.stride_tricks import as_strided\r\n",
        "def predict(audio, sr, step_size=10):\r\n",
        "    if len(audio.shape) == 2:\r\n",
        "        audio = audio.mean(1)  # make mono\r\n",
        "    audio = audio.astype(np.float32)\r\n",
        "    if sr != model_srate:\r\n",
        "        # resample audio if necessary\r\n",
        "        from resampy import resample\r\n",
        "        audio = resample(audio, sr, model_srate)\r\n",
        "\r\n",
        "    # make 1024-sample frames of the audio with hop length of 10 milliseconds\r\n",
        "    hop_length = int(model_srate * step_size / 1000)\r\n",
        "    n_frames = 1 + int((len(audio) - 1024) / hop_length)\r\n",
        "    frames = as_strided(audio, shape=(1024, n_frames),\r\n",
        "                        strides=(audio.itemsize, hop_length * audio.itemsize))\r\n",
        "    frames = frames.transpose().copy()\r\n",
        "\r\n",
        "    # normalize each frame -- this is expected by the model\r\n",
        "    frames -= np.mean(frames, axis=1)[:, np.newaxis]\r\n",
        "    frames /= np.std(frames, axis=1)[:, np.newaxis]\r\n",
        "\r\n",
        "    # run prediction and convert the frequency bin weights to Hz\r\n",
        "    return model(frames)"
      ],
      "outputs": [],
      "metadata": {
        "id": "Doqk900asdr_"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "source": [
        "def as_midi(pred):\r\n",
        "    # Convert from output buckets back to MIDI\r\n",
        "    midi = (pred.argmax(axis=1) / 5) + 24\r\n",
        "    return midi"
      ],
      "outputs": [],
      "metadata": {
        "id": "ojdkYbBisfIP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Steelpan-trained CREPE vs Original CREPE"
      ],
      "metadata": {
        "id": "26CgjLWCtHKw"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "source": [
        "import numpy as np\r\n",
        "\r\n",
        "model = make_model(model_size, [], \"new-crepe-\" + model_size + \".h5\")\r\n",
        "wav, sr = load_audio(\"tiny_16kHz/validation/62_train_sample_4.wav\")\r\n",
        "pred = as_midi(predict(wav, sr).numpy())\r\n",
        "print(pred)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[62.  62.  62.  62.  62.  62.  62.  62.  62.  62.  62.  62.  62.  62.\n",
            " 62.  62.  62.  62.  62.  62.  62.  62.  62.  62.  62.  62.  62.  62.\n",
            " 62.  62.  62.  62.  62.  62.  62.  62.  62.  62.  62.  62.  62.  62.\n",
            " 62.  62.  62.  62.  62.  62.  62.  62.  62.  62.  62.  62.  62.  62.\n",
            " 62.  62.  62.  62.  62.  62.  62.  62.  62.  62.  62.  62.  62.  62.\n",
            " 62.  62.  62.  62.  62.  62.  62.  62.  62.  62.  62.  62.  62.  62.\n",
            " 62.  62.  62.  62.  62.  62.  62.  62.  62.  62.  62.  62.  62.  62.\n",
            " 62.  62.  62.  62.  62.  62.  62.  62.  62.  62.  62.  62.  62.  62.\n",
            " 62.  62.  62.  62.  62.  62.  62.  62.  62.  74.  62.  62.  62.  62.\n",
            " 62.  62.  62.  62.  62.  62.  62.  62.  62.  62.  62.  62.  62.  74.\n",
            " 74.  74.  74.  62.  62.  62.  62.  62.  74.  74.  74.  74.  74.  74.\n",
            " 74.  74.  74.  74.  74.  74.  74.  74.  74.  74.  74.  74.  74.  74.\n",
            " 74.  74.  73.8 72.  72.  72.  72.  72.  72.  72.  72.  72.  72.  72.\n",
            " 72.  72.  72.  72.  72.  72.  72.  72.  72.  72.  72.  72.  72.  72.\n",
            " 72.  72.  72.  72.  72.  72.  72.  72.  72.  72.  72.  72.  72.  72.\n",
            " 72.  72.  72.  72.  72.  72.  72.  72.  72.  72.  72.  72.2]\n"
          ]
        }
      ],
      "metadata": {
        "id": "NaXp0A9Dsh1Q",
        "outputId": "592c6432-4c7a-4ab0-c712-049f6928c9fb"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [],
      "outputs": [],
      "metadata": {}
    }
  ]
}